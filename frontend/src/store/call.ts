import { create } from 'zustand';
import { io, Socket } from 'socket.io-client';
import { toast } from '../components/Toast';
import { parseApiError, getUserFriendlyMessage } from '../utils/errorHandler';
import { useAuthStore } from './auth';

// Use relative URL in production (when served from backend), absolute URL in development
const getSocketUrl = () => {
  if (import.meta.env.VITE_SOCKET_URL) {
    return import.meta.env.VITE_SOCKET_URL;
  }
  // In production, if served from same origin, use relative path
  if (import.meta.env.PROD) {
    return window.location.origin;
  }
  return 'http://localhost:3001';
};

const getApiUrl = () => {
  if (import.meta.env.VITE_API_URL) {
    return import.meta.env.VITE_API_URL;
  }
  // In production, if served from same origin, use relative path
  if (import.meta.env.PROD) {
    return window.location.origin;
  }
  return 'http://localhost:3001';
};

const SOCKET_URL = getSocketUrl();
const API_URL = getApiUrl();

interface TranscriptSegment {
  speaker: string;
  speakerId?: string;
  text: string;
  timestamp: number;
}

interface AINotes {
  summary: string;
  bullets: string[];
  actionItems: Array<{ text: string; assignee?: string }>;
  decisions: string[];
  suggestedReplies: string[];
  keyTopics: string[];
  isFinal?: boolean;
  lastUpdated?: number;
}

interface Participant {
  userId: string;
  userName: string;
  socketId: string;
  isMuted?: boolean; // Track mute state per participant (multi-party)
  isVideoOff?: boolean; // Track video state per participant (multi-party)
}

// Web Speech API types
declare global {
  interface Window {
    SpeechRecognition: any;
    webkitSpeechRecognition: any;
  }
}

interface CallState {
  socket: Socket | null;
  peerConnection: RTCPeerConnection | null; // Keep for backward compatibility (1-on-1 calls)
  peerConnections: Map<string, RTCPeerConnection>; // Map of socketId -> peer connection (for multiple participants)
  localStream: MediaStream | null;
  remoteStream: MediaStream | null; // Keep for backward compatibility (1-on-1 calls)
  remoteStreams: Map<string, MediaStream>; // Map of socketId -> remote stream (for multiple participants)
  speechRecognition: any | null;
  callRecorder: MediaRecorder | null; // For recording the call
  recordingChunks: Blob[]; // Store recording chunks
  
  roomId: string | null;
  callId: string | null;
  userName: string | null; // Store user name for transcript
  isHost: boolean;
  callStatus: 'idle' | 'connecting' | 'waiting' | 'active' | 'ended';
  participants: Participant[];
  callStartTime: number | null; // Timestamp when call became active (for continuous duration)
  
  transcript: TranscriptSegment[];
  interimTranscript: string; // For showing live interim results
  aiNotes: AINotes | null;
  aiAnalysisInterval: ReturnType<typeof setInterval> | null; // For periodic AI analysis
  
  isMuted: boolean;
  isVideoOff: boolean;
  isRecording: boolean;
  isMinimized: boolean; // For floating PiP overlay when navigating away
  
  error: string | null;
  
  initSocket: (token: string, userName: string) => void;
  disconnectSocket: () => void;
  createRoom: (token: string) => Promise<string>;
  joinRoom: (roomId: string, token: string) => Promise<void>;
  createPeerConnectionForParticipant: (socketId: string, localStream: MediaStream, participantName?: string) => Promise<RTCPeerConnection | null>;
  createOfferForParticipant: (socketId: string, pc: RTCPeerConnection) => Promise<void>;
  leaveRoom: () => void;
  endCall: () => void;
  toggleMute: () => void;
  toggleVideo: () => void;
  sendTranscript: (text: string) => void;
  requestNotes: () => void;
  startAIAnalysis: () => void;
  stopAIAnalysis: () => void;
  analyzeTranscript: (isFinal?: boolean) => Promise<void>;
  startSpeechRecognition: () => void;
  stopSpeechRecognition: () => void;
  startCallRecording: () => void;
  stopCallRecording: () => Promise<void>;
  clearCall: () => void;
  minimizeCall: () => void;
  maximizeCall: () => void;
}

const ICE_SERVERS: RTCConfiguration = {
  iceServers: [
    { urls: 'stun:stun.l.google.com:19302' },
    { urls: 'stun:stun1.l.google.com:19302' },
    { urls: 'stun:stun2.l.google.com:19302' },
    { urls: 'stun:stun3.l.google.com:19302' },
    { urls: 'stun:stun4.l.google.com:19302' },
  ],
  iceCandidatePoolSize: 10, // Pre-gather candidates for faster connection
};

export const useCallStore = create<CallState>((set, get) => ({
  socket: null,
  peerConnection: null,
  peerConnections: new Map<string, RTCPeerConnection>(),
  localStream: null,
  remoteStream: null,
  remoteStreams: new Map<string, MediaStream>(),
  speechRecognition: null,
  callRecorder: null,
  recordingChunks: [],
  
  roomId: null,
  callId: null,
  userName: null,
  isHost: false,
  callStatus: 'idle',
  participants: [],
  callStartTime: null,
  
  transcript: [],
  interimTranscript: '',
  aiNotes: null,
  aiAnalysisInterval: null,
  
  isMuted: false,
  isVideoOff: false,
  isRecording: false,
  isMinimized: false,
  
  error: null,

  initSocket: (token: string, userName: string) => {
    const existingSocket = get().socket;
    if (existingSocket?.connected) {
      console.log('[SOCKET] Already connected, skipping initialization');
      return;
    }

    // Disconnect existing socket if it exists but not connected
    if (existingSocket) {
      console.log('[SOCKET] Disconnecting existing socket');
      existingSocket.disconnect();
    }

    // Store user name for transcript
    set({ userName });

    const socket = io(SOCKET_URL, {
      auth: { token, userName },
      transports: ['websocket', 'polling'], // Fallback to polling if websocket fails
      reconnection: true,
      reconnectionDelay: 1000,
      reconnectionAttempts: 10, // Increase attempts for better reliability
      timeout: 20000, // 20 second connection timeout
      upgrade: true, // Allow transport upgrades
      rememberUpgrade: true, // Remember successful transport
      forceNew: false, // Reuse existing connection if available
    });
    
    // Store socket immediately so joinRoom can access it
    set({ socket });
    
    console.log('[SOCKET] Initializing connection to:', SOCKET_URL);
    console.log('[SOCKET] Socket instance created, waiting for connection...');

    // Track connection attempts to avoid showing errors too early
    let connectionAttempts = 0;
    let errorToastShown = false;

    socket.on('connect', () => {
      console.log('[SOCKET] ‚úÖ Connected successfully');
      console.log('[SOCKET] Socket ID:', socket.id);
      console.log('[SOCKET] Transport:', socket.io.engine?.transport?.name || 'unknown');
      set({ error: null });
      // Reset error tracking on successful connection
      connectionAttempts = 0;
      errorToastShown = false;
      // Don't show toast on every connect (too noisy, connection is expected)
      // toast.success('Connected', 'Socket connection established');
    });
    
    socket.on('connect_error', (error: any) => {
      connectionAttempts++;
      
      // Log error for debugging
      console.warn('[SOCKET] ‚ö†Ô∏è Connection error (will retry):', error.message, `Attempt ${connectionAttempts}`);
      
      // Transport errors are expected - Socket.IO will automatically try polling
      const isTransportError = error.type === 'TransportError' || 
                               error.message === 'websocket error' ||
                               error.message?.includes('websocket') ||
                               error.message?.includes('transport');
      
      if (isTransportError) {
        // Transport error - just log, Socket.IO will automatically try polling
        console.log('[SOCKET] WebSocket transport failed, will try polling fallback...');
        return; // Don't show error for transport failures
      }
      
      // For non-transport errors, only show error after multiple failed attempts
      // This gives Socket.IO time to retry and potentially succeed
      if (connectionAttempts >= 5 && !errorToastShown) {
        console.error('[SOCKET] ‚ùå Connection failed after multiple attempts:', {
          message: error.message,
          type: error.type || 'unknown',
          attempts: connectionAttempts,
        });
        
        errorToastShown = true;
        const message = `Unable to connect to call server: ${error.message || 'Connection failed'}`;
        set({ error: message });
        toast.error('Connection Error', message);
      }
    });
    
    // Reset error tracking on successful connection
    socket.on('connect', () => {
      connectionAttempts = 0;
      errorToastShown = false;
    });
    
    socket.on('reconnect', (attemptNumber) => {
      console.log('[SOCKET] ‚úÖ Reconnected after', attemptNumber, 'attempts');
      // Only show toast if it took multiple attempts (significant reconnection)
      if (attemptNumber > 3) {
        toast.success('Reconnected', 'Connection restored');
      }
      
      // Rejoin room if we were in one
      const { roomId } = get();
      if (roomId && socket.connected) {
        console.log('[SOCKET] Rejoining room after reconnect:', roomId);
        socket.emit('room:join', { roomId });
      }
    });
    
    socket.on('reconnect_error', (error) => {
      // Only log, don't show error - Socket.IO will keep trying
      console.warn('[SOCKET] ‚ö†Ô∏è Reconnection attempt failed (will retry):', error.message || error);
    });
    
    socket.on('reconnect_failed', () => {
      console.error('[SOCKET] ‚ùå Reconnection failed - giving up');
      toast.error('Connection Failed', 'Unable to reconnect. Please refresh the page.');
    });

    socket.on('disconnect', (reason) => {
      console.log('[SOCKET] Disconnected:', reason);
      // Only show warning for unexpected disconnects, not for manual disconnects
      if (reason === 'io server disconnect' || reason === 'transport close') {
        console.log('[SOCKET] Unexpected disconnect, will attempt to reconnect...');
        // Don't show toast for normal reconnection scenarios
      } else if (reason === 'io client disconnect') {
        console.log('[SOCKET] Client-initiated disconnect');
      }
    });

    socket.on('error', (error: any) => {
      console.error('Socket error:', error);
      const message = error.message || 'An error occurred';
      set({ error: message });
      toast.error('Error', message);
    });

    socket.on('room:join:error', (data: { error: string; message: string; maxParticipants: number }) => {
      console.error('[ROOM] ‚ùå Failed to join room:', data.message);
      set({ error: data.message, callStatus: 'idle' });
      toast.error('Room Full', data.message || `Maximum ${data.maxParticipants} participants allowed`);
    });

    socket.on('room:joined', async (data) => {
      // Filter out current user from participants list and remove duplicates
      const authUser = useAuthStore.getState().user;
      const currentUserId = authUser?._id;
      const currentSocketId = socket.id;
      
      // CRITICAL: Deduplicate participants by userId (UI-level deduplication)
      // This ensures no duplicate participants even if backend sends duplicates
      const participantMap = new Map<string, any>();
      (data.participants || []).forEach((p: any) => {
        // Filter out current user
        if (p.userId === currentUserId || p.socketId === currentSocketId) {
          return;
        }
        // Use userId as key for deduplication (most reliable identifier)
        const userId = p.userId || p._id;
        if (userId && !participantMap.has(userId)) {
          participantMap.set(userId, p);
        } else if (userId && participantMap.has(userId)) {
          // If duplicate found, keep the one with more complete data
          const existing = participantMap.get(userId);
          if (p.userName && !existing.userName) {
            participantMap.set(userId, p);
          }
        }
      });
      const filteredParticipants = Array.from(participantMap.values());
      
      console.log('[ROOM] Joined room with participants:', filteredParticipants.length);
      set({
        roomId: data.roomId,
        callId: data.callId,
        participants: filteredParticipants,
        callStatus: data.callStarted ? 'active' : 'waiting',
      });

      // For multi-party mesh: Create peer connections for all existing participants
      const { localStream, createPeerConnectionForParticipant, createOfferForParticipant } = get();
      if (localStream && filteredParticipants.length > 0) {
        console.log('[MULTI] üöÄ Creating peer connections for', filteredParticipants.length, 'existing participants');
        for (const participant of filteredParticipants) {
          try {
            const pc = await createPeerConnectionForParticipant(
              participant.socketId,
              localStream,
              participant.userName
            );
            if (pc) {
              // Create and send offer for this participant
              await createOfferForParticipant(participant.socketId, pc);
            }
          } catch (error: any) {
            console.error(`[MULTI] ‚ùå Error creating connection for ${participant.socketId}:`, error);
          }
        }
      }
    });

    socket.on('user:joined', async (data) => {
      // Don't add current user to participants list
      const currentSocketId = socket.id;
      if (data.socketId === currentSocketId) {
        console.log('[ROOM] Ignoring own join event');
        return;
      }
      
      console.log('[ROOM] üéâ ========== USER JOINED EVENT ==========');
      console.log('[ROOM] üéâ User details:', {
        socketId: data.socketId,
        userName: data.userName,
        userId: data.userId,
      });
      
      set((state) => {
        // CRITICAL: Deduplicate by userId (most reliable identifier)
        const userId = data.userId || data._id;
        const exists = state.participants.some(p => 
          (p.userId === userId) || (p.socketId === data.socketId)
        );
        if (exists) {
          console.log('[ROOM] ‚ö†Ô∏è Participant already exists:', { userId, socketId: data.socketId });
          // Update existing participant if new data is more complete
          return {
            participants: state.participants.map(p => 
              (p.userId === userId || p.socketId === data.socketId) 
                ? { ...p, ...data } // Merge new data
                : p
            ),
          };
        }
        console.log('[ROOM] ‚úÖ Adding new participant:', data.userName, data.socketId);
        // Initialize participant with default states (will be updated when tracks are received)
        return {
          participants: [...state.participants, {
            ...data,
            isMuted: false, // Default to unmuted (will be updated from track state)
            isVideoOff: false, // Default to video on (will be updated from track state)
          }],
        };
      });
      
      // Multi-party mesh: Create peer connection for the newly joined participant
      const { localStream, createPeerConnectionForParticipant, createOfferForParticipant } = get();
      if (localStream) {
        try {
          console.log('[MULTI] üéâ Creating peer connection for newly joined participant:', data.socketId);
          const pc = await createPeerConnectionForParticipant(
            data.socketId,
            localStream,
            data.userName
          );
          if (pc) {
            // Create and send offer to the new participant
            await createOfferForParticipant(data.socketId, pc);
          }
        } catch (error: any) {
          console.error('[MULTI] ‚ùå Error creating connection for new participant:', error);
          toast.error('Connection Error', 'Failed to establish video connection');
        }
      } else {
        console.warn('[MULTI] ‚ö†Ô∏è Cannot create peer connection - local stream not ready');
      }
    });

    socket.on('user:left', (data) => {
      console.log('[CALL] User left:', data.userName, 'Remaining participants:', data.participantCount);
      
      // Close peer connection for this participant (multi-party mesh)
      const { peerConnections } = get();
      const pc = peerConnections.get(data.socketId);
      if (pc) {
        console.log(`[MULTI] üóëÔ∏è Closing peer connection for ${data.socketId}`);
        pc.close();
        set((state) => {
          const newPeerConnections = new Map(state.peerConnections);
          newPeerConnections.delete(data.socketId);
          const newRemoteStreams = new Map(state.remoteStreams);
          newRemoteStreams.delete(data.socketId);
          return { 
            peerConnections: newPeerConnections,
            remoteStreams: newRemoteStreams,
          };
        });
      }
      
      set((state) => {
        const updatedParticipants = state.participants.filter(p => p.socketId !== data.socketId);
        console.log('[CALL] Updated participants count:', updatedParticipants.length);
        return {
          participants: updatedParticipants,
        };
      });
      
      // Keep call active - don't change call status
      // Call only ends when call:ended event is received (last participant left)
      const { callStatus } = get();
      if (callStatus === 'active') {
        console.log('[CALL] Call continues with remaining participants');
      }
    });

    socket.on('signal:offer', async (data) => {
      // Multi-party: Get or create peer connection for this specific participant
      const { peerConnections, localStream, createPeerConnectionForParticipant } = get();
      if (!localStream) {
        console.error('[MULTI] ‚ùå Cannot handle offer - no local stream');
        return;
      }

      let pc = peerConnections.get(data.fromId);
      if (!pc) {
        console.log(`[MULTI] üÜï Creating peer connection for offer from ${data.fromId}`);
        const newPc = await createPeerConnectionForParticipant(data.fromId, localStream, data.userName);
        if (!newPc) {
          console.error('[MULTI] ‚ùå Failed to create peer connection');
          return;
        }
        pc = newPc;
      }
      
      try {
        console.log('[MULTI] üì• Received offer from:', data.fromId);
        
        // Set remote description
        await pc.setRemoteDescription(new RTCSessionDescription(data.offer));
        
        // Create and send answer
        const answer = await pc.createAnswer({
          offerToReceiveAudio: true,
          offerToReceiveVideo: true,
        });
        await pc.setLocalDescription(answer);
        
        socket.emit('signal:answer', {
          targetId: data.fromId,
          answer: pc.localDescription,
        });
        console.log(`[MULTI] ‚úÖ Answer sent to ${data.fromId}`);
      } catch (error: any) {
        console.error(`[MULTI] ‚ùå Error handling offer from ${data.fromId}:`, error);
        toast.error('Connection Error', 'Failed to accept video connection');
      }
    });

    socket.on('signal:answer', async (data) => {
      // Multi-party: Get peer connection for this specific participant
      const { peerConnections } = get();
      const pc = peerConnections.get(data.fromId);
      
      if (!pc) {
        console.error(`[MULTI] ‚ùå Cannot handle answer - no peer connection for ${data.fromId}`);
        return;
      }
      
      try {
        console.log(`[MULTI] üì• Received answer from ${data.fromId}`);
        
        if (pc.signalingState === 'have-local-offer' || pc.signalingState === 'stable') {
          await pc.setRemoteDescription(new RTCSessionDescription(data.answer));
          console.log(`[MULTI] ‚úÖ Answer processed for ${data.fromId}`);
        } else {
          console.warn(`[MULTI] ‚ö†Ô∏è Wrong signaling state for ${data.fromId}:`, pc.signalingState);
        }
      } catch (error: any) {
        console.error(`[MULTI] ‚ùå Error handling answer from ${data.fromId}:`, error);
      }
    });

    socket.on('signal:candidate', async (data) => {
      // Multi-party: Get peer connection for this specific participant
      const { peerConnections } = get();
      const pc = peerConnections.get(data.fromId);
      
      if (!pc) {
        console.warn(`[MULTI] ‚ö†Ô∏è Cannot add ICE candidate - no peer connection for ${data.fromId}`);
        return;
      }
      
      try {
        if (data.candidate) {
          console.log(`[MULTI] üì• Received ICE candidate from ${data.fromId}`);
          await pc.addIceCandidate(new RTCIceCandidate(data.candidate));
          console.log(`[MULTI] ‚úÖ ICE candidate added for ${data.fromId}`);
        }
      } catch (error: any) {
        // Ignore duplicate/invalid candidate errors
        if (!error.message?.includes('duplicate') && !error.message?.includes('Invalid')) {
          console.error(`[MULTI] ‚ùå Error adding ICE candidate from ${data.fromId}:`, error);
        }
      }
    });

    socket.on('call:started', (data) => {
      console.log('[CALL] üìû Call started event received:', data);
      const startTime = Date.now();
      set({
        callStatus: 'active',
        callId: data.callId,
        isRecording: true, // Set flag, actual recording will start when streams are ready
        callStartTime: startTime, // Store start time for continuous duration
      });
      // Start speech recognition for transcription
      // Ensure socket is connected first
      const { socket: currentSocket } = get();
      if (currentSocket && currentSocket.connected) {
        setTimeout(() => {
          console.log('[CALL] Starting speech recognition after call:started');
          get().startSpeechRecognition();
          // CRITICAL: Start call recording automatically when call starts
          console.log('[RECORDING] Auto-starting recording after call:started event');
          get().startCallRecording();
        }, 1000); // Wait 1 second for streams to be ready
      } else {
        console.warn('[CALL] ‚ö†Ô∏è Socket not connected, cannot start transcription');
        // Wait for socket and retry
        const checkSocketInterval = setInterval(() => {
          const { socket: checkSocket, localStream } = get();
          if (checkSocket && checkSocket.connected && localStream) {
            console.log('[CALL] Socket connected and streams ready, starting services');
            get().startSpeechRecognition();
            // CRITICAL: Start call recording when socket connects and streams are ready
            console.log('[RECORDING] Auto-starting recording after socket connection');
            get().startCallRecording();
            get().startAIAnalysis();
            clearInterval(checkSocketInterval);
          }
        }, 500);
        setTimeout(() => clearInterval(checkSocketInterval), 10000);
      }
    });

    socket.on('call:ended', async (data) => {
      console.log('[CALL] Call ended for all participants:', data?.reason || 'unknown');
      set({
        callStatus: 'ended',
        isRecording: false,
      });
      // Stop speech recognition
      get().stopSpeechRecognition();
      // Stop AI analysis
      get().stopAIAnalysis();
      // Generate final summary (only when call actually ends for everyone)
      await get().analyzeTranscript(true);
      // Stop and upload recording (only when call actually ends for everyone)
      await get().stopCallRecording();
    });

    socket.on('transcript:chunk', (segment: TranscriptSegment) => {
      console.log('[TRANSCRIPT] ‚úÖ Received transcript chunk:', {
        speaker: segment.speaker,
        speakerId: segment.speakerId,
        text: segment.text.substring(0, 100) + (segment.text.length > 100 ? '...' : ''),
        fullTextLength: segment.text.length,
        timestamp: segment.timestamp,
        currentTime: Date.now(),
        timeDiff: Date.now() - segment.timestamp,
      });
      const authUser = useAuthStore.getState().user;
      const currentUserId = authUser?._id;
      console.log('[TRANSCRIPT] Current user ID:', currentUserId);
      console.log('[TRANSCRIPT] Current transcript length:', get().transcript.length);
      console.log('[TRANSCRIPT] Socket connection state:', socket.connected ? 'connected' : 'disconnected');
      
      // Get current user info to check if this is from current user
      const { userName: currentUserName } = get();
      
      // Normalize speaker name - keep original speaker name from server
      // Server already identifies the speaker correctly, so we should display it as-is
      // Only normalize if it's clearly the current user
      let displaySpeaker = segment.speaker;
      if (segment.speakerId && currentUserId) {
        const segmentUserId = segment.speakerId.toString();
        const currentUserIdStr = currentUserId.toString();
        if (segmentUserId === currentUserIdStr) {
          // This is from current user - use their name or "You"
          displaySpeaker = currentUserName || 'You';
        } else {
          // This is from another user - keep their name as sent by server
          displaySpeaker = segment.speaker;
        }
      }
      
      const normalizedSegment: TranscriptSegment = {
        ...segment,
        speaker: displaySpeaker,
      };
      
      set((state) => {
        // Check if this segment already exists (avoid duplicates)
        // Check by speaker + text + timestamp to be more accurate
        const exists = state.transcript.some(
          s => {
            const sameText = s.text.trim().toLowerCase() === segment.text.trim().toLowerCase();
            const sameSpeaker = s.speaker === displaySpeaker || 
              (segment.speakerId && s.speakerId && s.speakerId.toString() === segment.speakerId.toString());
            const closeTimestamp = Math.abs(s.timestamp - segment.timestamp) < 10000;
            return sameText && sameSpeaker && closeTimestamp;
          }
        );
        if (exists) {
          console.log('[TRANSCRIPT] ‚ö†Ô∏è Duplicate segment, skipping');
          return state;
        }
        
        console.log('[TRANSCRIPT] ‚úÖ Adding new segment from:', displaySpeaker);
        const newTranscript = [...state.transcript, normalizedSegment];
        console.log('[TRANSCRIPT] ‚úÖ Updated transcript length:', newTranscript.length);
        return {
          transcript: newTranscript,
          interimTranscript: '', // Clear interim when final arrives
        };
      });
      
      // Auto-scroll transcript to bottom
      setTimeout(() => {
        const transcriptElement = document.querySelector('[data-transcript-container]');
        if (transcriptElement) {
          transcriptElement.scrollTop = transcriptElement.scrollHeight;
        }
      }, 100);
    });

    // Handle remote participant audio state changes
    socket.on('participant:audio:changed', (data: { socketId: string; userId: string; userName: string; isMuted: boolean }) => {
      console.log('[AUDIO] üì• Remote participant audio state changed:', {
        socketId: data.socketId,
        userId: data.userId,
        userName: data.userName,
        isMuted: data.isMuted,
      });
      
      // Update participant state
      set((state) => ({
        participants: state.participants.map((p) =>
          p.socketId === data.socketId || p.userId === data.userId
            ? { ...p, isMuted: data.isMuted }
            : p
        ),
      }));
      
      // Update remote audio element mute state if needed
      const remoteAudioElement = document.getElementById(`remote-audio-${data.socketId}`) as HTMLAudioElement;
      if (remoteAudioElement) {
        // Note: We can't directly mute remote audio, but we can show visual indicator
        console.log(`[AUDIO] Remote participant ${data.userId} is ${data.isMuted ? 'muted' : 'unmuted'}`);
      }
    });

    // Handle remote participant video state changes
    socket.on('participant:video:changed', (data: { socketId: string; userId: string; userName: string; isVideoOff: boolean }) => {
      console.log('[VIDEO] üì• Remote participant video state changed:', {
        socketId: data.socketId,
        userId: data.userId,
        userName: data.userName,
        isVideoOff: data.isVideoOff,
      });
      
      // Update participant state
      set((state) => ({
        participants: state.participants.map((p) =>
          p.socketId === data.socketId || p.userId === data.userId
            ? { ...p, isVideoOff: data.isVideoOff }
            : p
        ),
      }));
    });

    socket.on('ai:notes', (notes: AINotes) => {
      set({ aiNotes: notes });
    });

    set({ socket });
  },

  disconnectSocket: () => {
    const { socket, peerConnection, localStream } = get();
    
    if (localStream) {
      localStream.getTracks().forEach(track => track.stop());
    }
    
    if (peerConnection) {
      peerConnection.close();
    }
    
    if (socket) {
      socket.disconnect();
    }
    
    set({
      socket: null,
      peerConnection: null,
      localStream: null,
      remoteStream: null,
    });
  },

  createRoom: async (token: string) => {
    set({ error: null });
    
    try {
      const response = await fetch(`${API_URL}/api/rooms`, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${token}`,
          'Content-Type': 'application/json',
        },
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        
        // If token is invalid, clear storage and reload
        if (response.status === 401) {
          localStorage.removeItem('accessToken');
          localStorage.removeItem('refreshToken');
          localStorage.removeItem('user');
          toast.error('Session Expired', 'Please login again.');
          window.location.href = '/login';
          throw new Error('Session expired. Please login again.');
        }
        
        const error = parseApiError(errorData);
        const message = getUserFriendlyMessage(error);
        set({ error: message });
        toast.error('Failed to Create Room', message);
        throw new Error(message);
      }

      const data = await response.json();
      set({ isHost: true, callId: data.callId, error: null });
      toast.success('Room Created', `Room code: ${data.roomId}`);
      return data.roomId;
    } catch (error: any) {
      // Network error - check if server is reachable
      if (error instanceof TypeError && error.message === 'Failed to fetch') {
        console.error('[ROOM] Network error - server may not be running on', API_URL);
        const message = 'Unable to connect to server. Please ensure the backend is running.';
        set({ error: message });
        toast.error('Connection Error', message);
        throw new Error(message);
      }
      console.error('Create room error:', error);
      throw error;
    }
  },

  // Helper function to create a peer connection for a specific participant (multi-party mesh topology)
  createPeerConnectionForParticipant: async (socketId: string, localStream: MediaStream, participantName?: string) => {
    const { socket, peerConnections, remoteStreams } = get();
    if (!socket || !localStream) {
      console.error('[MULTI] ‚ùå Cannot create peer connection - missing socket or stream');
      return null;
    }

    // Check if connection already exists
    if (peerConnections.has(socketId)) {
      console.log(`[MULTI] ‚ö†Ô∏è Peer connection already exists for ${socketId}, reusing`);
      return peerConnections.get(socketId)!;
    }

    console.log(`[MULTI] üÜï Creating new peer connection for participant: ${socketId} (${participantName || 'unknown'})`);
    const pc = new RTCPeerConnection(ICE_SERVERS);

    // Add local tracks
    localStream.getTracks().forEach(track => {
      try {
        pc.addTrack(track, localStream);
        console.log(`[MULTI] ‚úÖ Added ${track.kind} track to peer connection for ${socketId}`);
      } catch (error: any) {
        console.error(`[MULTI] ‚ùå Error adding ${track.kind} track:`, error.message);
      }
    });

    // Handle remote tracks - store per participant
    pc.ontrack = (event) => {
      if (!event.track) return;
      
      console.log(`[MULTI] üìπ Received track from ${socketId}:`, event.track.kind, {
        enabled: event.track.enabled,
        readyState: event.track.readyState,
        muted: event.track.muted,
      });
      
      // Get or create remote stream for this participant
      let remoteStream = remoteStreams.get(socketId);
      if (!remoteStream) {
        remoteStream = new MediaStream();
        set((state) => {
          const newRemoteStreams = new Map(state.remoteStreams);
          newRemoteStreams.set(socketId, remoteStream!);
          return { remoteStreams: newRemoteStreams };
        });
      }

      // Add track if not already present
      if (!remoteStream.getTracks().find(t => t.id === event.track.id)) {
        remoteStream.addTrack(event.track);
        console.log(`[MULTI] ‚úÖ Added ${event.track.kind} track to remote stream for ${socketId}`);
        
        // CRITICAL: Update participant state based on actual track state
        // This ensures UI shows correct video/audio status
        const track = event.track;
        if (track.kind === 'video') {
          const isVideoOff = !track.enabled;
          console.log(`[MULTI] üìπ Detected video state for ${socketId}:`, { isVideoOff, enabled: track.enabled });
          set((state) => {
            const updatedParticipants = state.participants.map(p => 
              p.socketId === socketId 
                ? { ...p, isVideoOff } 
                : p
            );
            return { participants: updatedParticipants };
          });
        } else if (track.kind === 'audio') {
          const isMuted = !track.enabled;
          console.log(`[MULTI] üé§ Detected audio state for ${socketId}:`, { isMuted, enabled: track.enabled });
          set((state) => {
            const updatedParticipants = state.participants.map(p => 
              p.socketId === socketId 
                ? { ...p, isMuted } 
                : p
            );
            return { participants: updatedParticipants };
          });
        }
        
        // Monitor track state changes
        const handleTrackEnabledChange = () => {
          if (track.kind === 'video') {
            const isVideoOff = !track.enabled;
            console.log(`[MULTI] üìπ Video track state changed for ${socketId}:`, { isVideoOff });
            set((state) => {
              const updatedParticipants = state.participants.map(p => 
                p.socketId === socketId 
                  ? { ...p, isVideoOff } 
                  : p
              );
              return { participants: updatedParticipants };
            });
          } else if (track.kind === 'audio') {
            const isMuted = !track.enabled;
            console.log(`[MULTI] üé§ Audio track state changed for ${socketId}:`, { isMuted });
            set((state) => {
              const updatedParticipants = state.participants.map(p => 
                p.socketId === socketId 
                  ? { ...p, isMuted } 
                  : p
              );
              return { participants: updatedParticipants };
            });
          }
        };
        
        // Listen for track state changes
        track.addEventListener('ended', handleTrackEnabledChange);
        track.addEventListener('mute', handleTrackEnabledChange);
        track.addEventListener('unmute', handleTrackEnabledChange);
        
        // Update store to trigger re-render
        set((state) => {
          const newRemoteStreams = new Map(state.remoteStreams);
          newRemoteStreams.set(socketId, remoteStream!);
          return { remoteStreams: newRemoteStreams };
        });
      }
    };

    // Handle ICE candidates - send to specific participant
    pc.onicecandidate = (event) => {
      if (event.candidate && socket.connected) {
        console.log(`[MULTI] üì§ Sending ICE candidate to ${socketId}`);
        socket.emit('signal:candidate', {
          targetId: socketId,
          candidate: event.candidate,
        });
      }
    };

    // Handle connection state changes
    pc.onconnectionstatechange = () => {
      console.log(`[MULTI] üîÑ Connection state for ${socketId}:`, pc.connectionState);
      
      if (pc.connectionState === 'connected') {
        const { callStartTime } = get();
        const startTime = callStartTime || Date.now();
        set({ 
          callStatus: 'active', 
          callStartTime: startTime,
        });
      } else if (pc.connectionState === 'failed' || pc.connectionState === 'disconnected') {
        console.warn(`[MULTI] ‚ö†Ô∏è Connection to ${socketId} failed/disconnected`);
      }
    };

    // Store peer connection
    set((state) => {
      const newPeerConnections = new Map(state.peerConnections);
      newPeerConnections.set(socketId, pc);
      return { peerConnections: newPeerConnections };
    });

    return pc;
  },

  // Helper to create offer for a specific participant
  createOfferForParticipant: async (socketId: string, pc: RTCPeerConnection) => {
    const { socket } = get();
    if (!socket || !pc) return;

    try {
      console.log(`[MULTI] üéØ Creating offer for ${socketId}`);
      const offer = await pc.createOffer({
        offerToReceiveAudio: true,
        offerToReceiveVideo: true,
      });
      await pc.setLocalDescription(offer);
      
      socket.emit('signal:offer', {
        targetId: socketId,
        offer: pc.localDescription,
      });
      console.log(`[MULTI] ‚úÖ Offer sent to ${socketId}`);
    } catch (error: any) {
      console.error(`[MULTI] ‚ùå Error creating offer for ${socketId}:`, error);
    }
  },

  joinRoom: async (roomId: string, token: string) => {
    set({ error: null, callStatus: 'connecting' });
    
    // Wait for socket to be connected (with timeout)
    const waitForSocket = (): Promise<Socket> => {
      return new Promise((resolve, reject) => {
        const currentSocket = get().socket;
        if (currentSocket && currentSocket.connected) {
          resolve(currentSocket);
          return;
        }

        // Wait up to 10 seconds for socket connection
        const timeout = setTimeout(() => {
          reject(new Error('Socket connection timeout. Please refresh and try again.'));
        }, 10000);

        // Check every 100ms
        const checkInterval = setInterval(() => {
          const checkSocket = get().socket;
          if (checkSocket && checkSocket.connected) {
            clearTimeout(timeout);
            clearInterval(checkInterval);
            resolve(checkSocket);
          }
        }, 100);

        // If socket doesn't exist yet, wait a bit for initSocket to create it
        if (!currentSocket) {
          setTimeout(() => {
            const newSocket = get().socket;
            if (newSocket) {
              newSocket.once('connect', () => {
                clearTimeout(timeout);
                clearInterval(checkInterval);
                resolve(newSocket);
              });
            }
          }, 100);
        } else {
          // Socket exists but not connected, wait for connect event
          currentSocket.once('connect', () => {
            clearTimeout(timeout);
            clearInterval(checkInterval);
            resolve(currentSocket);
          });
        }
      });
    };

    try {
      // Wait for socket connection before proceeding
      console.log('[JOIN] Waiting for socket connection...');
      await waitForSocket();
      console.log('[JOIN] ‚úÖ Socket connected, proceeding with room join');

      const response = await fetch(`${API_URL}/api/rooms/${roomId}/join`, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${token}`,
          'Content-Type': 'application/json',
        },
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        
        if (response.status === 401) {
          localStorage.removeItem('accessToken');
          localStorage.removeItem('refreshToken');
          localStorage.removeItem('user');
          toast.error('Session Expired', 'Please login again.');
          window.location.href = '/login';
          throw new Error('Session expired');
        }
        
        if (response.status === 404) {
          const message = 'Room not found. Please check the room code.';
          set({ error: message });
          toast.error('Room Not Found', message);
          throw new Error(message);
        }
        
        const error = parseApiError(errorData);
        const message = getUserFriendlyMessage(error);
        set({ error: message });
        toast.error('Failed to Join Room', message);
        throw new Error(message);
      }

      const data = await response.json();
      set({
        isHost: data.isHost,
        callId: data.callId,
        roomId,
        error: null,
      });
    } catch (error: any) {
      if (error instanceof TypeError && error.message === 'Failed to fetch') {
        console.error('[ROOM] Network error - server may not be running on', API_URL);
        const message = 'Unable to connect to server. Please ensure the backend is running.';
        set({ error: message, callStatus: 'idle' });
        toast.error('Connection Error', message);
        throw new Error(message);
      }
      if (error.message?.includes('timeout')) {
        set({ error: error.message, callStatus: 'idle' });
        toast.error('Connection Timeout', error.message);
        throw error;
      }
      set({ callStatus: 'idle' });
      throw error;
    }

    // Get media devices with proper error handling
    let stream: MediaStream | null = null;
    try {
      console.log('[WEBRTC] Requesting camera and microphone permissions...');
      stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
        },
        video: {
          width: { ideal: 1280 },
          height: { ideal: 720 },
          facingMode: 'user',
        },
      });
      
      console.log('[WEBRTC] ‚úÖ Media stream obtained:', {
        id: stream.id,
        active: stream.active,
        videoTracks: stream.getVideoTracks().length,
        audioTracks: stream.getAudioTracks().length,
        videoEnabled: stream.getVideoTracks()[0]?.enabled,
        audioEnabled: stream.getAudioTracks()[0]?.enabled,
      });

      // Verify tracks are actually enabled
      stream.getVideoTracks().forEach(track => {
        console.log('[WEBRTC] Video track:', {
          id: track.id,
          enabled: track.enabled,
          readyState: track.readyState,
          muted: track.muted,
        });
      });

      stream.getAudioTracks().forEach(track => {
        console.log('[WEBRTC] Audio track:', {
          id: track.id,
          enabled: track.enabled,
          readyState: track.readyState,
          muted: track.muted,
        });
      });
    } catch (error: any) {
      console.error('[WEBRTC] ‚ùå Error getting user media:', error);
      let errorMessage = 'Failed to access camera/microphone';
      
      if (error.name === 'NotAllowedError' || error.name === 'PermissionDeniedError') {
        errorMessage = 'Camera/microphone permission denied. Please allow access and refresh.';
      } else if (error.name === 'NotFoundError' || error.name === 'DevicesNotFoundError') {
        errorMessage = 'No camera/microphone found. Please connect a device.';
      } else if (error.name === 'NotReadableError' || error.name === 'TrackStartError') {
        errorMessage = 'Camera/microphone is being used by another application.';
      } else if (error.name === 'OverconstrainedError') {
        errorMessage = 'Camera does not support required settings.';
      }
      
      set({ error: errorMessage, callStatus: 'idle' });
      toast.error('Media Access Error', errorMessage);
      throw new Error(errorMessage);
    }

    if (!stream) {
      throw new Error('Failed to get media stream');
    }

    // For multi-party mesh topology, we'll create peer connections per participant
    // Don't create a single peerConnection here - create them when participants join
    console.log('[MULTI] üöÄ Prepared for multi-party mesh topology');

    // Tracks will be added when creating peer connections for each participant
    console.log('[MULTI] üì§ Local stream ready:', {
      videoTracks: stream.getVideoTracks().length,
      audioTracks: stream.getAudioTracks().length,
    });

    // Old single peerConnection handlers removed - now handled per participant in helper function
    // Multi-party mesh topology uses separate peer connections per participant
    
    /* REMOVED: Single peerConnection handlers - replaced with per-participant handlers
    pc.ontrack = (event) => {
      console.log('[WEBRTC] üìπ ========== ONTRACK EVENT ==========');
      console.log('[WEBRTC] üìπ Full event details:', {
        streams: event.streams?.length || 0,
        streamIds: event.streams?.map(s => s.id) || [],
        track: event.track ? {
          kind: event.track.kind,
          id: event.track.id,
          enabled: event.track.enabled,
          readyState: event.track.readyState,
          muted: event.track.muted,
          label: event.track.label,
        } : null,
        receiver: event.receiver ? {
          track: event.receiver.track?.kind,
        } : null,
        transceiver: event.transceiver ? {
          mid: event.transceiver.mid,
          direction: event.transceiver.direction,
          currentDirection: event.transceiver.currentDirection,
        } : null,
      });
      
      // CRITICAL: Handle track from event
      if (!event.track) {
        console.error('[WEBRTC] ‚ùå ontrack event has no track - cannot process');
        return;
      }
      
      const track = event.track;
      console.log('[WEBRTC] üéØ Processing track:', {
        kind: track.kind,
        id: track.id,
        enabled: track.enabled,
        readyState: track.readyState,
        muted: track.muted,
      });
      
      // CRITICAL: Get existing stream from store or create new one
      // Multiple ontrack events will fire (one for audio, one for video)
      // We need to merge them into a single stream
      const { remoteStream: existingRemoteStream } = get();
      
      if (!mergedRemoteStream) {
        // Use existing stream from store if available, otherwise create new
        if (existingRemoteStream && existingRemoteStream.active) {
          console.log('[WEBRTC] ‚ôªÔ∏è Reusing existing remote stream from store');
          mergedRemoteStream = existingRemoteStream;
        } else {
          console.log('[WEBRTC] üÜï Creating new merged remote stream');
          mergedRemoteStream = new MediaStream();
        }
      }
      
      // Check if this track is already in the merged stream
      const existingTrack = mergedRemoteStream.getTracks().find(t => t.id === track.id);
      if (existingTrack) {
        console.log('[WEBRTC] ‚ÑπÔ∏è Track already in merged stream, skipping:', {
          trackId: track.id,
          kind: track.kind,
        });
        
        // Even if track exists, update store to trigger re-render
        // This ensures React components see the updated stream
        set({ remoteStream: mergedRemoteStream });
        return;
      } else {
        console.log('[WEBRTC] ‚ûï Adding track to merged stream:', {
          trackId: track.id,
          kind: track.kind,
        });
        mergedRemoteStream.addTrack(track);
        
        // CRITICAL: Create a new stream object with all tracks to ensure React detects the change
        // This is necessary because React uses object reference equality
        const allTracks = mergedRemoteStream.getTracks();
        const newStream = new MediaStream(allTracks);
        mergedRemoteStream = newStream;
        console.log('[WEBRTC] ‚úÖ Created new stream object with all tracks:', {
          streamId: newStream.id,
          totalTracks: allTracks.length,
          trackIds: allTracks.map(t => ({ kind: t.kind, id: t.id })),
        });
      }
      
      // CRITICAL: Verify video track exists and is enabled
      const videoTracks = mergedRemoteStream.getVideoTracks();
      const audioTracks = mergedRemoteStream.getAudioTracks();
      
      console.log('[WEBRTC] üìä Merged remote stream analysis:', {
        streamId: mergedRemoteStream.id,
        totalTracks: mergedRemoteStream.getTracks().length,
        videoTracks: videoTracks.length,
        audioTracks: audioTracks.length,
        streamActive: mergedRemoteStream.active,
      });
      
      if (videoTracks.length > 0) {
        const videoTrack = videoTracks[0];
        console.log('[WEBRTC] üé• Remote video track details:', {
          id: videoTrack.id,
          enabled: videoTrack.enabled,
          readyState: videoTrack.readyState,
          muted: videoTrack.muted,
          label: videoTrack.label,
          settings: videoTrack.getSettings(),
        });
        
        // CRITICAL: Ensure track is enabled
        if (!videoTrack.enabled) {
          console.warn('[WEBRTC] ‚ö†Ô∏è Remote video track is disabled, enabling...');
          videoTrack.enabled = true;
          console.log('[WEBRTC] ‚úÖ Video track enabled');
        }
        
        // Monitor track state changes
        const streamId = mergedRemoteStream.id; // Capture stream ID for closure
        const handleTrackEnded = () => {
          console.log('[WEBRTC] ‚ö†Ô∏è Remote video track ended');
          const { remoteStream: currentStream } = get();
          if (currentStream && currentStream.id === streamId) {
            set({ remoteStream: null });
            mergedRemoteStream = null;
          }
        };
        
        const handleTrackMute = () => {
          console.log('[WEBRTC] ‚ö†Ô∏è Remote video track muted');
        };
        
        const handleTrackUnmute = () => {
          console.log('[WEBRTC] ‚úÖ Remote video track unmuted');
          const { remoteStream: currentStream } = get();
          if (currentStream && currentStream.id === streamId && mergedRemoteStream) {
            set({ remoteStream: mergedRemoteStream });
          }
        };
        
        // Remove old listeners if they exist
        videoTrack.removeEventListener('ended', handleTrackEnded);
        videoTrack.removeEventListener('mute', handleTrackMute);
        videoTrack.removeEventListener('unmute', handleTrackUnmute);
        
        // Add new listeners
        videoTrack.addEventListener('ended', handleTrackEnded);
        videoTrack.addEventListener('mute', handleTrackMute);
        videoTrack.addEventListener('unmute', handleTrackUnmute);
      } else {
        console.log('[WEBRTC] ‚ÑπÔ∏è Remote stream has no video tracks yet (audio-only track received)');
      }
      
      // CRITICAL: Set remote stream in store IMMEDIATELY after each track is added
      // This ensures React components get the updated stream with all tracks
      console.log('[WEBRTC] üíæ Setting merged remote stream in store...');
      set({ remoteStream: mergedRemoteStream });
      console.log('[WEBRTC] ‚úÖ Merged remote stream set in store:', {
        streamId: mergedRemoteStream.id,
        videoTracks: videoTracks.length,
        audioTracks: audioTracks.length,
        shouldTriggerVideoDisplay: videoTracks.length > 0,
      });
      
      // Force update after a short delay to ensure React re-renders
      setTimeout(() => {
        const { remoteStream: currentStream } = get();
        if (currentStream && currentStream.id === mergedRemoteStream?.id) {
          const currentVideoTracks = currentStream.getVideoTracks();
          console.log('[WEBRTC] ‚úÖ Remote stream confirmed in store after delay:', {
            streamId: currentStream.id,
            videoTracks: currentVideoTracks.length,
            audioTracks: currentStream.getAudioTracks().length,
            videoTrackEnabled: currentVideoTracks[0]?.enabled,
            videoTrackReadyState: currentVideoTracks[0]?.readyState,
          });
        } else {
          console.warn('[WEBRTC] ‚ö†Ô∏è Stream mismatch or not set in store:', {
            expectedId: mergedRemoteStream?.id,
            currentId: currentStream?.id,
          });
        }
      }, 100);
      
      console.log('[WEBRTC] üìπ ========== ONTRACK EVENT COMPLETE ==========');
    }; */

    /* REMOVED: Old onicecandidate - now handled per participant
    pc.onicecandidate = (event) => {
        if (event.candidate) {
          console.log('[WEBRTC] üì§ ICE candidate generated:', {
            candidate: event.candidate.candidate?.substring(0, 100) + '...',
            sdpMLineIndex: event.candidate.sdpMLineIndex,
            sdpMid: event.candidate.sdpMid,
          });
          
          const { socket: currentSocket, participants } = get();
          if (currentSocket && currentSocket.connected) {
            // Send to all existing participants
            if (participants.length > 0) {
              participants.forEach(p => {
                console.log('[WEBRTC] üì§ Sending ICE candidate to participant:', p.socketId);
                currentSocket.emit('signal:candidate', {
                  targetId: p.socketId,
                  candidate: event.candidate,
                });
              });
            } else {
              // No participants yet - this is normal, candidates will be sent when participants join
              // ICE candidates are generated continuously, so we'll catch up when participants arrive
              console.log('[WEBRTC] üì¶ ICE candidate generated but no participants yet (will send when participants join)');
            }
          } else {
            console.warn('[WEBRTC] ‚ö†Ô∏è Socket not connected, cannot send ICE candidate');
          }
        } else {
          // null candidate means end of candidates
          console.log('[WEBRTC] ‚úÖ All ICE candidates gathered (null candidate received)');
          
          // Send null candidate to all participants to signal end of candidates
          const { socket: currentSocket, participants } = get();
          if (currentSocket && currentSocket.connected && participants.length > 0) {
            participants.forEach(p => {
              console.log('[WEBRTC] üì§ Sending null ICE candidate (end of candidates) to:', p.socketId);
              currentSocket.emit('signal:candidate', {
                targetId: p.socketId,
                candidate: null,
              });
            });
          }
        }
      }; */

    /* REMOVED: Old onconnectionstatechange - now handled per participant
    pc.onconnectionstatechange = () => {
        console.log('[WEBRTC] üîÑ Connection state changed:', {
          connectionState: pc.connectionState,
          iceConnectionState: pc.iceConnectionState,
          iceGatheringState: pc.iceGatheringState,
          signalingState: pc.signalingState,
        });
        
        if (pc.connectionState === 'connected') {
          console.log('[WEBRTC] ‚úÖ Peer connection CONNECTED - video should be working now');
          const { callStartTime } = get();
          // Only set start time if not already set (preserve existing start time)
          const startTime = callStartTime || Date.now();
          set({ 
            callStatus: 'active', 
            isRecording: true,
            callStartTime: startTime, // Store start time for continuous duration
          });
      // Start speech recognition for transcription when connected
          // But only if socket is also connected and user is not muted
      setTimeout(() => {
            const { socket: currentSocket, isMuted } = get();
            if (currentSocket && currentSocket.connected && !isMuted) {
          console.log('[SPEECH] Starting speech recognition after WebRTC connection');
          get().startSpeechRecognition();
          // Start call recording
          get().startCallRecording();
        } else {
              console.warn('[SPEECH] ‚ö†Ô∏è Socket not connected or user is muted, waiting...');
              // Wait for socket connection and unmute
              const checkSocketInterval = setInterval(() => {
                const { socket: checkSocket, isMuted: checkMuted } = get();
                if (checkSocket && checkSocket.connected && !checkMuted) {
                  console.log('[SPEECH] Socket connected and unmuted, starting recognition now');
              get().startSpeechRecognition();
              get().startCallRecording();
                  clearInterval(checkSocketInterval);
            }
          }, 500);
          
          // Stop checking after 10 seconds
              setTimeout(() => clearInterval(checkSocketInterval), 10000);
        }
      }, 1000); // Wait 1 second for stream to stabilize
        } else if (pc.connectionState === 'failed') {
          toast.error('Connection Failed', 'Unable to establish call connection');
          set({ error: 'Connection failed', callStatus: 'idle' });
          get().stopSpeechRecognition();
        } else if (pc.connectionState === 'disconnected') {
          toast.warning('Connection Lost', 'Trying to reconnect...');
          get().stopSpeechRecognition();
        }
      };

      pc.onicecandidateerror = (event) => {
        console.error('ICE candidate error:', event);
      }; */

      // Set local stream (peer connections created per participant)
      set({
        localStream: stream,
        // CRITICAL: Initialize mute/video state from actual track state
        // If no track exists, default to muted/off (true). Otherwise, check if track is enabled.
        isMuted: stream.getAudioTracks().length === 0 || !stream.getAudioTracks()[0]?.enabled,
        isVideoOff: stream.getVideoTracks().length === 0 || !stream.getVideoTracks()[0]?.enabled,
      });

      // Ensure socket is connected before emitting room:join
      const { socket: currentSocket } = get();
      if (currentSocket && currentSocket.connected) {
        console.log('[JOIN] ‚úÖ Emitting room:join with connected socket');
        currentSocket.emit('room:join', { roomId });
        
        // CRITICAL: Broadcast initial video/audio state immediately after joining
        // This ensures other participants know the actual state (not defaulting to "off")
        setTimeout(() => {
          const { socket: checkSocket, localStream, roomId: currentRoomId } = get();
          if (checkSocket && checkSocket.connected && localStream && currentRoomId === roomId) {
            // Get actual track states (more reliable than store state)
            const audioTrack = localStream.getAudioTracks()[0];
            const videoTrack = localStream.getVideoTracks()[0];
            const actualIsMuted = !audioTrack || !audioTrack.enabled;
            const actualIsVideoOff = !videoTrack || !videoTrack.enabled;
            
            console.log('[JOIN] üì§ Broadcasting initial media state:', {
              isMuted: actualIsMuted,
              isVideoOff: actualIsVideoOff,
            });
            
            // Broadcast current state to all participants
            checkSocket.emit('participant:audio:toggle', { isMuted: actualIsMuted });
            checkSocket.emit('participant:video:toggle', { isVideoOff: actualIsVideoOff });
          }
        }, 500); // Small delay to ensure room:join is processed first
      } else {
        console.warn('[JOIN] ‚ö†Ô∏è Socket not connected, waiting...');
        // Wait for socket and then emit
        const waitAndEmit = setInterval(() => {
          const { socket: checkSocket } = get();
          if (checkSocket && checkSocket.connected) {
            console.log('[JOIN] ‚úÖ Socket connected, emitting room:join now');
            checkSocket.emit('room:join', { roomId });
            
            // Broadcast initial state after joining
            setTimeout(() => {
              const { socket: emitSocket, localStream, roomId: currentRoomId } = get();
              if (emitSocket && emitSocket.connected && localStream && currentRoomId === roomId) {
                const audioTrack = localStream.getAudioTracks()[0];
                const videoTrack = localStream.getVideoTracks()[0];
                const actualIsMuted = !audioTrack || !audioTrack.enabled;
                const actualIsVideoOff = !videoTrack || !videoTrack.enabled;
                
                console.log('[JOIN] üì§ Broadcasting initial media state (delayed):', {
                  isMuted: actualIsMuted,
                  isVideoOff: actualIsVideoOff,
                });
                
                emitSocket.emit('participant:audio:toggle', { isMuted: actualIsMuted });
                emitSocket.emit('participant:video:toggle', { isVideoOff: actualIsVideoOff });
              }
            }, 500);
            
            clearInterval(waitAndEmit);
          }
        }, 100);
        setTimeout(() => clearInterval(waitAndEmit), 10000); // Timeout after 10s
      }

      // Start speech recognition immediately after getting stream (only if not muted)
      // This allows transcription even before other participants join
      // IMPORTANT: Both users need speech recognition running to see each other's transcripts
      // Each user's browser captures their own microphone, so both must have recognition active
      setTimeout(() => {
        const { isMuted, speechRecognition, callStatus } = get();
        if (!isMuted && !speechRecognition && callStatus !== 'ended') {
          console.log('[JOIN] ‚úÖ Starting speech recognition after getting media stream');
          console.log('[JOIN] ‚ö†Ô∏è IMPORTANT: Both users must have speech recognition active to see each other\'s transcripts');
        get().startSpeechRecognition();
        } else if (isMuted) {
          console.log('[JOIN] ‚ö†Ô∏è User is muted, skipping speech recognition (will start when unmuted)');
          console.log('[JOIN] ‚ö†Ô∏è REMINDER: Unmute to enable transcription for this user');
        } else if (speechRecognition) {
          console.log('[JOIN] ‚úÖ Speech recognition already active');
        } else if (callStatus === 'ended') {
          console.log('[JOIN] ‚ö†Ô∏è Call ended, not starting speech recognition');
        }
      }, 1000);
  },

  leaveRoom: async () => {
    const { socket, roomId, localStream, peerConnections } = get();
    
    console.log('[LEAVE] Leaving room:', roomId);
    
    // Stop speech recognition first
    get().stopSpeechRecognition();
    // Stop recording
    await get().stopCallRecording();
    
    // Emit leave event (but don't disconnect socket - let it reconnect)
    if (socket && roomId && socket.connected) {
      console.log('[LEAVE] Emitting room:leave event');
      socket.emit('room:leave');
    }
    
    // Stop all media tracks immediately
    if (localStream) {
      localStream.getTracks().forEach(track => {
        track.stop();
        console.log('[LEAVE] Stopped track:', track.kind);
      });
    }
    
    // Close all peer connections (multi-party mesh)
    console.log(`[MULTI] üóëÔ∏è Closing ${peerConnections.size} peer connections`);
    peerConnections.forEach((pc, socketId) => {
      pc.close();
      console.log(`[MULTI] ‚úÖ Closed peer connection for ${socketId}`);
    });
    
    // Don't disconnect socket - let it stay connected for reconnection
    // Only disconnect if explicitly requested (e.g., logout)
    
    // Reset call state but keep socket for potential reconnection
    set({
      peerConnection: null, // Keep for backward compatibility
      peerConnections: new Map(),
      localStream: null,
      remoteStream: null, // Keep for backward compatibility
      remoteStreams: new Map(),
      roomId: null,
      callId: null,
      isHost: false,
      callStatus: 'idle',
      participants: [],
      transcript: [],
      interimTranscript: '',
      aiNotes: null,
      isMuted: false,
      isVideoOff: false,
      isRecording: false,
      error: null,
    });
    
    console.log('[LEAVE] Room left, socket remains connected for reconnection');
  },

  endCall: async () => {
    const { socket, localStream, peerConnections, roomId } = get();
    
    console.log('[CALL] User leaving call, but call continues for others');
    
    // Stop speech recognition for this user only
    get().stopSpeechRecognition();
    // Stop AI analysis for this user only
    get().stopAIAnalysis();
    
    // Stop all media tracks for this user
    if (localStream) {
      localStream.getTracks().forEach(track => {
        track.stop();
        console.log('[CALL] Stopped track:', track.kind);
      });
    }
    
    // Close all peer connections (multi-party mesh)
    peerConnections.forEach((pc, socketId) => {
      pc.close();
      console.log(`[MULTI] ‚úÖ Closed peer connection for ${socketId}`);
    });
    
    // Emit call:end to remove this user from the call
    // The call will continue for other participants
    if (socket && roomId) {
      socket.emit('call:end');
    }
    
    // Leave the room (this will trigger user:left event for others)
    get().leaveRoom();
    
    // Clear local state (this user has left, but call may continue for others)
    set({ 
      callStatus: 'ended', // This user's call has ended
      isRecording: false,
      isMinimized: false,
      callStartTime: null,
      localStream: null,
      remoteStream: null,
      remoteStreams: new Map(),
      peerConnection: null,
      peerConnections: new Map(),
      participants: [], // Clear participants list for this user
    });
  },

  toggleMute: () => {
    const { localStream, speechRecognition, socket, roomId, peerConnection, peerConnections } = get();
    
    if (!localStream) {
      console.warn('[MUTE] ‚ö†Ô∏è No local stream available');
      return;
    }
    
    // Get actual track state (more reliable than store state)
    const audioTracks = localStream.getAudioTracks();
    if (audioTracks.length === 0) {
      console.warn('[MUTE] ‚ö†Ô∏è No audio tracks available');
      return;
    }
    
    const audioTrack = audioTracks[0];
    
    // Check current track state BEFORE toggling
    const currentTrackEnabled = audioTrack.enabled;
    const newTrackEnabled = !currentTrackEnabled;
    const newMutedState = !newTrackEnabled; // Muted = track disabled
    
    console.log('[MUTE] üîÑ Before toggle - Track enabled:', currentTrackEnabled, 'State muted:', !currentTrackEnabled);
    
    // CRITICAL: Update track state (enabled = unmuted, disabled = muted)
    audioTrack.enabled = newTrackEnabled;
    
    console.log('[MUTE] ‚úÖ After toggle - Track enabled:', audioTrack.enabled, 'State muted:', newMutedState);
    
    // Update all audio tracks in the stream
    audioTracks.forEach(track => {
      if (track.id !== audioTrack.id) {
        track.enabled = newTrackEnabled;
      }
    });
    
    // Update peer connection senders to reflect track state
    if (peerConnection) {
      const senders = peerConnection.getSenders();
      const audioSender = senders.find(s => s.track?.kind === 'audio');
      if (audioSender && audioSender.track) {
        // Ensure sender track state matches
        audioSender.track.enabled = newTrackEnabled;
        console.log('[MUTE] üì° Peer connection audio sender track enabled:', audioSender.track.enabled);
      }
    }
    
    // Update all peer connections (for multi-participant calls)
    peerConnections.forEach((pc, socketId) => {
      const senders = pc.getSenders();
      const audioSender = senders.find(s => s.track?.kind === 'audio');
      if (audioSender && audioSender.track) {
        audioSender.track.enabled = newTrackEnabled;
        console.log(`[MUTE] üì° Peer connection ${socketId} audio sender track enabled:`, audioSender.track.enabled);
      }
    });
    
    // Update store state to match track state IMMEDIATELY
    set({ isMuted: newMutedState });
    
    // Notify other participants of audio state change
    if (socket && roomId) {
      socket.emit('participant:audio:toggle', { isMuted: newMutedState });
      console.log('[MUTE] üì§ Emitted state change to room:', { roomId, isMuted: newMutedState });
    }
    
    // Stop speech recognition when muted, restart when unmuted
    if (newMutedState) {
      // Muted - stop recognition
      if (speechRecognition) {
        console.log('[MUTE] ‚è∏Ô∏è Stopping speech recognition (muted)');
        get().stopSpeechRecognition();
      }
    } else {
      // Unmuted - start recognition if call is active
      const { callStatus } = get();
      if (callStatus === 'active' && !speechRecognition) {
        console.log('[MUTE] ‚ñ∂Ô∏è Starting speech recognition (unmuted)');
        setTimeout(() => {
          get().startSpeechRecognition();
        }, 500);
      }
    }
  },

  toggleVideo: () => {
    const { localStream, socket, roomId, peerConnection, peerConnections } = get();
    
    if (!localStream) {
      console.warn('[VIDEO] ‚ö†Ô∏è No local stream available');
      return;
    }
    
    // Get actual track state (more reliable than store state)
    const videoTracks = localStream.getVideoTracks();
    if (videoTracks.length === 0) {
      console.warn('[VIDEO] ‚ö†Ô∏è No video tracks available');
      return;
    }
    
    const videoTrack = videoTracks[0];
    
    // Check current track state BEFORE toggling
    const currentTrackEnabled = videoTrack.enabled;
    const newTrackEnabled = !currentTrackEnabled;
    const newVideoOff = !newTrackEnabled; // Video off = track disabled
    
    console.log('[VIDEO] üîÑ Before toggle - Track enabled:', currentTrackEnabled, 'State videoOff:', !currentTrackEnabled);
    
    // CRITICAL: Update track state (enabled = video on, disabled = video off)
    videoTrack.enabled = newTrackEnabled;
    
    console.log('[VIDEO] ‚úÖ After toggle - Track enabled:', videoTrack.enabled, 'State videoOff:', newVideoOff);
    
    // Update all video tracks in the stream
    videoTracks.forEach(track => {
      if (track.id !== videoTrack.id) {
        track.enabled = newTrackEnabled;
      }
    });
    
    // Update peer connection senders to reflect track state
    if (peerConnection) {
      const senders = peerConnection.getSenders();
      const videoSender = senders.find(s => s.track?.kind === 'video');
      if (videoSender && videoSender.track) {
        // Ensure sender track state matches
        videoSender.track.enabled = newTrackEnabled;
        console.log('[VIDEO] üì° Peer connection video sender track enabled:', videoSender.track.enabled);
      }
    }
    
    // Update all peer connections (for multi-participant calls)
    peerConnections.forEach((pc, socketId) => {
      const senders = pc.getSenders();
      const videoSender = senders.find(s => s.track?.kind === 'video');
      if (videoSender && videoSender.track) {
        videoSender.track.enabled = newTrackEnabled;
        console.log(`[VIDEO] üì° Peer connection ${socketId} video sender track enabled:`, videoSender.track.enabled);
      }
    });
    
    // Update store state to match track state IMMEDIATELY
    set({ isVideoOff: newVideoOff });
    
    // Notify other participants of video state change
    if (socket && roomId) {
      socket.emit('participant:video:toggle', { isVideoOff: newVideoOff });
      console.log('[VIDEO] üì§ Emitted state change to room:', { roomId, isVideoOff: newVideoOff });
    }
  },

  sendTranscript: (text: string) => {
    const { socket } = get();
    
    if (socket && text.trim()) {
      socket.emit('transcript:manual', {
        text: text.trim(),
        timestamp: Date.now(),
      });
    }
  },

  requestNotes: () => {
    const { socket } = get();
    
    if (socket) {
      socket.emit('notes:request');
    }
  },

  startAIAnalysis: () => {
    const { aiAnalysisInterval, callStatus } = get();
    
    // Don't start if already running or call not active
    if (aiAnalysisInterval || callStatus !== 'active') {
      return;
    }

    console.log('[AI] Starting real-time AI analysis (every 30 seconds)');
    
    // Start immediate analysis
    get().analyzeTranscript(false);
    
    // Set up interval for periodic analysis
    const interval = setInterval(() => {
      const { callStatus: currentStatus } = get();
      if (currentStatus === 'active') {
        get().analyzeTranscript(false);
      } else {
        // Stop if call ended
        get().stopAIAnalysis();
      }
    }, 30000); // Every 30 seconds

    set({ aiAnalysisInterval: interval });
  },

  stopAIAnalysis: () => {
    const { aiAnalysisInterval } = get();
    
    if (aiAnalysisInterval) {
      clearInterval(aiAnalysisInterval);
      set({ aiAnalysisInterval: null });
      console.log('[AI] Stopped real-time AI analysis');
    }
  },

  analyzeTranscript: async (isFinal: boolean = false) => {
    const { transcript, callId, participants, callStartTime } = get();
    const { accessToken } = useAuthStore.getState();
    
    if (!callId || !accessToken) {
      console.warn('[AI] Cannot analyze: missing callId or accessToken');
      return;
    }

    if (transcript.length === 0) {
      console.log('[AI] No transcript to analyze yet');
      return;
    }

    try {
      // Convert transcript segments to text
      const transcriptText = transcript
        .map(seg => `${seg.speaker}: ${seg.text}`)
        .join('\n');

      // Calculate duration
      const duration = callStartTime ? Date.now() - callStartTime : 0;

      // Get participant names
      const participantNames = participants.map(p => p.userName);
      const authUser = useAuthStore.getState().user;
      if (authUser?.name && !participantNames.includes(authUser.name)) {
        participantNames.push(authUser.name);
      }

      console.log('[AI] Analyzing transcript...', {
        transcriptLength: transcriptText.length,
        segmentCount: transcript.length,
        isFinal,
      });

      const response = await fetch(`${API_URL}/api/calls/${callId}/analyze-transcript`, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${accessToken}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          transcript: transcriptText,
          participants: participantNames,
          duration,
          isFinal,
        }),
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        console.error('[AI] Analysis failed:', errorData);
        return;
      }

      const analysis = await response.json();
      
      // Update AI notes in store
      set({
        aiNotes: {
          summary: analysis.summary || '',
          bullets: analysis.keyPoints || [],
          actionItems: analysis.actionItems || [],
          decisions: analysis.decisions || [],
          suggestedReplies: analysis.nextSteps || [],
          keyTopics: analysis.topics || [],
          isFinal,
          lastUpdated: Date.now(),
        },
      });

      console.log('[AI] ‚úÖ Analysis complete', { isFinal });
    } catch (error: any) {
      console.error('[AI] Error analyzing transcript:', error);
    }
  },

  startSpeechRecognition: () => {
    const { socket, speechRecognition } = get();
    
    // Don't start if already active
    if (speechRecognition) {
      console.log('[SPEECH] Speech recognition already active');
      return;
    }
    
    // Allow speech recognition even without socket (for local display)
    if (!socket || !socket.connected) {
      console.warn('[SPEECH] ‚ö†Ô∏è No socket connection - will work locally only');
      // Continue anyway - we'll show transcript locally
    }
    
    // Check Web Speech API support
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    if (!SpeechRecognition) {
      console.error('[SPEECH] ‚ùå Web Speech API not supported in this browser');
      console.error('[SPEECH] Available:', {
        SpeechRecognition: typeof window.SpeechRecognition,
        webkitSpeechRecognition: typeof window.webkitSpeechRecognition,
        userAgent: navigator.userAgent,
      });
      toast.error('Speech Recognition Not Supported', 'Please use Chrome or Edge browser. Web Speech API is not available in this browser.');
      return;
    }
    
    console.log('[SPEECH] ‚úÖ Web Speech API is available');
    
    try {
      const recognition = new SpeechRecognition();
      
      // Configuration
      recognition.continuous = true; // Keep listening
      recognition.interimResults = true; // Show interim results
      recognition.lang = 'en-US'; // Language
      recognition.maxAlternatives = 1; // Only get best result
      
      recognition.onstart = () => {
        console.log('[SPEECH] ‚úÖ Speech recognition started');
        console.log('[SPEECH] Language:', recognition.lang);
        console.log('[SPEECH] Continuous:', recognition.continuous);
        console.log('[SPEECH] Interim results:', recognition.interimResults);
        console.log('[SPEECH] Max alternatives:', recognition.maxAlternatives);
        set({ speechRecognition: recognition }); // Store immediately
        toast.success('Speech Recognition Active', 'Listening for speech...');
      };
      
      recognition.onresult = (event: any) => {
        let finalTranscript = '';
        let interimText = '';
        
        console.log('[SPEECH] üìù onresult event:', {
          resultIndex: event.resultIndex,
          resultsLength: event.results.length,
        });
        
        // Process all results
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const result = event.results[i];
          const transcript = result[0]?.transcript || '';
          const isFinal = result.isFinal;
          
          console.log('[SPEECH] Result', i, ':', {
            transcript,
            isFinal,
            confidence: result[0]?.confidence,
          });
          
          if (isFinal) {
            finalTranscript += transcript + ' ';
          } else {
            interimText += transcript + ' ';
          }
        }
        
        // Update interim transcript in UI immediately for live feedback
        if (interimText.trim()) {
          console.log('[SPEECH] üìù Interim transcript:', interimText.trim());
          set({ interimTranscript: interimText.trim() });
        } else if (finalTranscript.trim()) {
          // Clear interim when we have final
          set({ interimTranscript: '' });
        }
        
        // Send final transcript to server and show locally as fallback
        if (finalTranscript.trim()) {
          console.log('[SPEECH] ‚úÖ Final transcript:', finalTranscript.trim());
          const { socket: currentSocket, roomId, isMuted, userName: currentUserName } = get();
          const authUser = useAuthStore.getState().user;
          
          // Don't send if muted
          if (isMuted) {
            console.log('[SPEECH] ‚ö†Ô∏è User is muted, ignoring transcript');
            return;
          }
          
          // Create segment for local display (fallback if server doesn't respond)
          const localSegment: TranscriptSegment = {
            speaker: currentUserName || 'You',
            speakerId: authUser?._id,
            text: finalTranscript.trim(),
            timestamp: Date.now(),
          };
          
          // Show locally immediately for better UX
          set((state) => {
            // Check if already exists
            const exists = state.transcript.some(
              s => s.text.trim().toLowerCase() === localSegment.text.trim().toLowerCase() 
                && Math.abs(s.timestamp - localSegment.timestamp) < 5000
            );
            if (!exists) {
              console.log('[SPEECH] ‚úÖ Adding transcript locally (will be replaced by server response)');
              return {
                transcript: [...state.transcript, localSegment],
                interimTranscript: '', // Clear interim
              };
            }
            return state;
          });
          
          // Send to server - server will broadcast back and replace/update the local one
          if (currentSocket && currentSocket.connected && roomId) {
            console.log('[SPEECH] üì§ Sending transcript to server:', {
              text: finalTranscript.trim(),
              roomId,
              socketConnected: currentSocket.connected,
            });
            
            try {
              currentSocket.emit('transcript:manual', {
                text: finalTranscript.trim(),
                timestamp: Date.now(),
              }, (response: any) => {
                if (response) {
                  console.log('[SPEECH] ‚úÖ Server acknowledged:', response);
                }
              });
            } catch (error: any) {
              console.error('[SPEECH] ‚ùå Error sending to server:', error.message);
            }
          } else {
            console.warn('[SPEECH] ‚ö†Ô∏è Socket not available - showing transcript locally only');
            if (!currentSocket) {
              console.warn('[SPEECH]   - No socket instance');
            } else if (!currentSocket.connected) {
              console.warn('[SPEECH]   - Socket not connected');
            } else if (!roomId) {
              console.warn('[SPEECH]   - No roomId');
            }
          }
          
          finalTranscript = '';
        }
      };
      
      recognition.onerror = (event: any) => {
        console.error('[SPEECH] Recognition error:', event.error);
        console.error('[SPEECH] Error details:', event);
        
        if (event.error === 'no-speech') {
          // This is normal, just continue - don't log as error
          console.log('[SPEECH] No speech detected (normal - will continue listening)');
          return;
        }
        
        if (event.error === 'audio-capture') {
          console.error('[SPEECH] ‚ùå Microphone not accessible');
          toast.error('Microphone Error', 'Could not access microphone. Please check permissions.');
          // Try to restart after a delay
          setTimeout(() => {
            const { callStatus } = get();
            if (callStatus === 'active') {
              console.log('[SPEECH] Attempting to restart after audio-capture error');
              get().startSpeechRecognition();
            }
          }, 2000);
        } else if (event.error === 'not-allowed') {
          console.error('[SPEECH] ‚ùå Microphone permission denied');
          toast.error('Permission Denied', 'Microphone permission required. Please allow microphone access.');
          set({ speechRecognition: null });
        } else if (event.error === 'network') {
          console.error('[SPEECH] ‚ùå Network error');
          // Don't show error toast for network issues - might be temporary
          console.log('[SPEECH] Network error - will retry on next recognition cycle');
        } else if (event.error === 'service-not-allowed') {
          console.error('[SPEECH] ‚ùå Speech recognition service not allowed');
          toast.error('Service Not Allowed', 'Please enable speech recognition in browser settings');
          set({ speechRecognition: null });
        } else if (event.error === 'aborted') {
          // Aborted is usually intentional, don't treat as error
          console.log('[SPEECH] Recognition aborted (normal)');
          return;
        } else {
          console.error('[SPEECH] ‚ùå Unknown error:', event.error);
          // Don't show toast for unknown errors - might be temporary
          console.log('[SPEECH] Will continue listening despite error');
        }
        
        // Only stop on critical errors that can't be recovered
        if (event.error === 'not-allowed' || event.error === 'service-not-allowed') {
          set({ speechRecognition: null });
        }
      };
      
      recognition.onend = () => {
        console.log('[SPEECH] Speech recognition ended');
        const { callStatus, speechRecognition: currentRec, localStream, isMuted } = get();
        
        // Don't restart if muted
        if (isMuted) {
          console.log('[SPEECH] ‚è∏Ô∏è Not restarting - user is muted');
          if (currentRec === recognition) {
            set({ speechRecognition: null });
          }
          return;
        }
        
        // Auto-restart if call is still active (Web Speech API auto-stops after ~60s of silence)
        if (callStatus === 'active' && currentRec === recognition && localStream) {
          console.log('[SPEECH] üîÑ Auto-restarting recognition (normal behavior - Web Speech API auto-stops)...');
          setTimeout(() => {
            const { speechRecognition: stillActive, callStatus: stillActiveStatus, localStream: stillStream, isMuted: stillMuted } = get();
            // Only restart if no other recognition instance is active, call is still active, and not muted
            if (!stillActive && stillActiveStatus === 'active' && stillStream && !stillMuted) {
              console.log('[SPEECH] ‚úÖ Restarting speech recognition');
              try {
              get().startSpeechRecognition();
              } catch (error) {
                console.error('[SPEECH] Error restarting recognition:', error);
                // Try again after a longer delay
                setTimeout(() => {
                  const { callStatus: retryStatus, localStream: retryStream, isMuted: retryMuted } = get();
                  if (retryStatus === 'active' && retryStream && !retryMuted) {
                    console.log('[SPEECH] Retrying recognition restart...');
                    get().startSpeechRecognition();
                  }
                }, 2000);
              }
            } else {
              console.log('[SPEECH] ‚è∏Ô∏è Not restarting - recognition already active, call ended, no stream, or muted');
            }
          }, 300); // Wait 300ms before restarting (faster restart)
        } else {
          console.log('[SPEECH] ‚èπÔ∏è Not restarting - call status:', callStatus, 'has stream:', !!localStream);
          if (currentRec === recognition) {
          set({ speechRecognition: null });
          }
        }
      };
      
      // Start recognition
      try {
      recognition.start();
      set({ speechRecognition: recognition });
      console.log('[SPEECH] ‚úÖ Started Web Speech API recognition');
      } catch (error: any) {
        // If already started, that's okay
        if (error.message?.includes('already started') || error.name === 'InvalidStateError') {
          console.log('[SPEECH] Recognition already started, continuing...');
          set({ speechRecognition: recognition });
        } else {
          throw error;
        }
      }
      
    } catch (error: any) {
      console.error('[SPEECH] Failed to start speech recognition:', error.message);
      toast.error('Failed to Start Recognition', error.message);
      set({ speechRecognition: null });
    }
  },

  stopSpeechRecognition: () => {
    const { speechRecognition } = get();
    
    if (speechRecognition) {
      try {
        speechRecognition.stop();
        set({ speechRecognition: null });
        console.log('[SPEECH] ‚úÖ Speech recognition stopped');
      } catch (error) {
        console.error('[SPEECH] Error stopping recognition:', error);
        set({ speechRecognition: null });
      }
    }
  },

  startCallRecording: () => {
    const { localStream, remoteStream, callId, callRecorder } = get();
    
    // Don't start if already recording
    if (callRecorder && callRecorder.state !== 'inactive') {
      console.log('[RECORDING] Already recording, state:', callRecorder.state);
      return;
    }
    
    if (!callId) {
      console.log('[RECORDING] ‚ö†Ô∏è No call ID, skipping recording');
      return;
    }
    
    if (!localStream) {
      console.log('[RECORDING] ‚ö†Ô∏è No local stream available');
      return;
    }
    
    // Check MediaRecorder support
    if (!window.MediaRecorder) {
      console.warn('[RECORDING] ‚ùå MediaRecorder not supported in this browser');
      toast.error('Recording Not Supported', 'MediaRecorder API is not available');
      return;
    }
    
    try {
      console.log('[RECORDING] üé¨ Starting call recording...');
      console.log('[RECORDING] Streams available:', {
        hasLocalStream: !!localStream,
        localVideoTracks: localStream.getVideoTracks().length,
        localAudioTracks: localStream.getAudioTracks().length,
        hasRemoteStream: !!remoteStream,
        remoteVideoTracks: remoteStream?.getVideoTracks().length || 0,
        remoteAudioTracks: remoteStream?.getAudioTracks().length || 0,
      });
      
      // CRITICAL: Combine local and remote streams for recording
      const mixedStream = new MediaStream();
      
      // Add local stream tracks (user's own video/audio)
      localStream.getTracks().forEach(track => {
        if (track.readyState === 'live' && (track.kind === 'audio' || track.kind === 'video')) {
          console.log('[RECORDING] ‚ûï Adding local track:', track.kind, track.id);
          mixedStream.addTrack(track);
        }
      });
      
      // Add remote stream tracks (other participants' video/audio)
      if (remoteStream) {
        remoteStream.getTracks().forEach(track => {
          if (track.readyState === 'live' && (track.kind === 'audio' || track.kind === 'video')) {
            console.log('[RECORDING] ‚ûï Adding remote track:', track.kind, track.id);
            mixedStream.addTrack(track);
          }
        });
      }
      
      const totalTracks = mixedStream.getTracks().length;
      const videoTracks = mixedStream.getVideoTracks().length;
      const audioTracks = mixedStream.getAudioTracks().length;
      
      console.log('[RECORDING] üìä Mixed stream tracks:', {
        totalTracks,
        videoTracks,
        audioTracks,
      });
      
      if (totalTracks === 0) {
        console.warn('[RECORDING] ‚ö†Ô∏è No active tracks available for recording');
        toast.warning('Recording Warning', 'No active media tracks to record');
        return;
      }
      
      // Find supported mime type (prefer VP9 for better compression)
      const supportedTypes = [
        'video/webm;codecs=vp9,opus', // Best compression
        'video/webm;codecs=vp8,opus', // Good compression
        'video/webm', // Fallback
        'audio/webm;codecs=opus', // Audio-only fallback
      ];
      
      let selectedMimeType = '';
      for (const mimeType of supportedTypes) {
        if (MediaRecorder.isTypeSupported(mimeType)) {
          selectedMimeType = mimeType;
          console.log('[RECORDING] ‚úÖ Selected codec:', mimeType);
          break;
        }
      }
      
      if (!selectedMimeType) {
        console.warn('[RECORDING] ‚ö†Ô∏è No supported codec found, using default');
        selectedMimeType = 'video/webm';
      }
      
      // Create MediaRecorder with selected codec
      const recorder = new MediaRecorder(mixedStream, {
        mimeType: selectedMimeType,
        videoBitsPerSecond: 2500000, // 2.5 Mbps for good quality/size balance
      });
      
      // CRITICAL: Clear previous chunks and store in state
      set({ recordingChunks: [] });
      
      recorder.ondataavailable = (event) => {
        if (event.data && event.data.size > 0) {
          console.log('[RECORDING] üì¶ Chunk received:', {
            size: event.data.size,
            type: event.data.type,
          });
          set((state) => ({
            recordingChunks: [...state.recordingChunks, event.data],
          }));
        }
      };
      
      recorder.onerror = (event: any) => {
        console.error('[RECORDING] ‚ùå Recording error:', event.error);
        toast.error('Recording Error', 'An error occurred while recording');
        set({ callRecorder: null, isRecording: false });
      };
      
      recorder.onstop = async () => {
        console.log('[RECORDING] ‚èπÔ∏è Recording stopped, preparing upload...');
        const { callId: currentCallId, recordingChunks: finalChunks } = get();
        
        if (finalChunks.length === 0) {
          console.warn('[RECORDING] ‚ö†Ô∏è No recording chunks collected');
          set({ callRecorder: null, isRecording: false, recordingChunks: [] });
          return;
        }
        
        if (!currentCallId) {
          console.error('[RECORDING] ‚ùå No call ID for upload');
          set({ callRecorder: null, isRecording: false, recordingChunks: [] });
          return;
        }
        
        try {
          console.log('[RECORDING] üì§ Creating blob from', finalChunks.length, 'chunks...');
          const blob = new Blob(finalChunks, { type: selectedMimeType || 'video/webm' });
          console.log('[RECORDING] ‚úÖ Blob created:', {
            size: blob.size,
            type: blob.type,
            sizeMB: (blob.size / (1024 * 1024)).toFixed(2),
          });
          
          const formData = new FormData();
          const filename = `recording-${currentCallId}-${Date.now()}.webm`;
          formData.append('recording', blob, filename);
          
          const token = localStorage.getItem('accessToken');
          if (!token) {
            console.error('[RECORDING] ‚ùå No access token available');
            toast.error('Upload Failed', 'Authentication required');
            set({ callRecorder: null, isRecording: false, recordingChunks: [] });
            return;
          }
          
          console.log('[RECORDING] üì§ Uploading to server...');
          const response = await fetch(`${API_URL}/api/calls/${currentCallId}/recording`, {
            method: 'POST',
            headers: {
              'Authorization': `Bearer ${token}`,
            },
            body: formData,
          });
          
          if (response.ok) {
            const result = await response.json();
            console.log('[RECORDING] ‚úÖ Recording uploaded successfully:', result);
            toast.success('Recording Saved', 'Call recording has been saved successfully');
          } else {
            const errorText = await response.text();
            console.error('[RECORDING] ‚ùå Upload failed:', {
              status: response.status,
              statusText: response.statusText,
              error: errorText,
            });
            toast.error('Upload Failed', `Could not upload recording: ${response.statusText}`);
          }
        } catch (error: any) {
          console.error('[RECORDING] ‚ùå Upload error:', error);
          toast.error('Upload Error', error.message || 'Failed to upload recording');
        } finally {
          // Clear recording state
          set({ callRecorder: null, isRecording: false, recordingChunks: [] });
        }
      };
      
      // Start recording with 1 second chunks (collects data every second)
      recorder.start(1000);
      set({ callRecorder: recorder, isRecording: true });
      console.log('[RECORDING] ‚úÖ Recording started successfully:', {
        state: recorder.state,
        mimeType: selectedMimeType,
        tracks: totalTracks,
      });
      toast.success('Recording Started', 'Call is being recorded');
      
    } catch (error: any) {
      console.error('[RECORDING] ‚ùå Failed to start recording:', {
        error: error.message,
        stack: error.stack,
      });
      toast.error('Recording Failed', error.message || 'Could not start recording');
      set({ callRecorder: null, isRecording: false });
    }
  },

  stopCallRecording: async () => {
    const { callRecorder } = get();
    
    if (!callRecorder) {
      console.log('[RECORDING] No active recording to stop');
      return;
    }
    
    if (callRecorder.state === 'inactive') {
      console.log('[RECORDING] Recording already stopped');
      set({ callRecorder: null, isRecording: false });
      return;
    }
    
    try {
      console.log('[RECORDING] ‚èπÔ∏è Stopping call recording, current state:', callRecorder.state);
      
      // Stop the recorder (this will trigger onstop event)
      callRecorder.stop();
      
      // Wait for onstop to complete (it handles the upload)
      // Give it enough time to process chunks and upload
      await new Promise(resolve => setTimeout(resolve, 2000));
      
      console.log('[RECORDING] ‚úÖ Recording stopped');
    } catch (error: any) {
      console.error('[RECORDING] ‚ùå Error stopping recording:', error);
      toast.error('Recording Error', 'Failed to stop recording properly');
      set({ callRecorder: null, isRecording: false, recordingChunks: [] });
    }
  },

  clearCall: async () => {
    // Stop speech recognition first
    get().stopSpeechRecognition();
    // Stop recording
    await get().stopCallRecording();
    
    // Make sure to stop any remaining tracks
    const { localStream, peerConnection, socket } = get();
    
    if (localStream) {
      localStream.getTracks().forEach(track => {
        track.stop();
      });
    }
    
    if (peerConnection) {
      peerConnection.close();
    }
    
    if (socket) {
      socket.disconnect();
    }
    
    set({
      socket: null,
      peerConnection: null,
      localStream: null,
      remoteStream: null,
      speechRecognition: null,
      callRecorder: null,
      recordingChunks: [],
      roomId: null,
      callId: null,
      userName: null,
      isHost: false,
      callStatus: 'idle',
      participants: [],
      callStartTime: null,
      transcript: [],
      interimTranscript: '',
      aiNotes: null,
      aiAnalysisInterval: null,
      isMuted: false,
      isVideoOff: false,
      isRecording: false,
      isMinimized: false,
      error: null,
    });
  },

  minimizeCall: () => {
    const { callStatus } = get();
    if (callStatus === 'active' || callStatus === 'waiting') {
      set({ isMinimized: true });
    }
  },

  maximizeCall: () => {
    set({ isMinimized: false });
  },
}));

