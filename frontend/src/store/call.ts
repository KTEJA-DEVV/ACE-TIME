import { create } from 'zustand';
import { io, Socket } from 'socket.io-client';
import { toast } from '../components/Toast';
import { parseApiError, getUserFriendlyMessage } from '../utils/errorHandler';
import { useAuthStore } from './auth';

// Use relative URL in production (when served from backend), absolute URL in development
const getSocketUrl = () => {
  if (import.meta.env.VITE_SOCKET_URL) {
    return import.meta.env.VITE_SOCKET_URL;
  }
  // In production, if served from same origin, use relative path
  if (import.meta.env.PROD) {
    return window.location.origin;
  }
  return 'http://localhost:3001';
};

const getApiUrl = () => {
  if (import.meta.env.VITE_API_URL) {
    return import.meta.env.VITE_API_URL;
  }
  // In production, if served from same origin, use relative path
  if (import.meta.env.PROD) {
    return window.location.origin;
  }
  return 'http://localhost:3001';
};

const SOCKET_URL = getSocketUrl();
const API_URL = getApiUrl();

interface TranscriptSegment {
  speaker: string;
  speakerId?: string;
  text: string;
  timestamp: number;
}

interface AINotes {
  summary: string;
  bullets: string[];
  actionItems: Array<{ text: string; assignee?: string }>;
  decisions: string[];
  suggestedReplies: string[];
  keyTopics: string[];
  isFinal?: boolean;
  lastUpdated?: number;
}

interface Participant {
  userId: string;
  userName: string;
  socketId: string;
}

// Web Speech API types
declare global {
  interface Window {
    SpeechRecognition: any;
    webkitSpeechRecognition: any;
  }
}

interface CallState {
  socket: Socket | null;
  peerConnection: RTCPeerConnection | null; // Keep for backward compatibility (1-on-1 calls)
  peerConnections: Map<string, RTCPeerConnection>; // Map of socketId -> peer connection (for multiple participants)
  localStream: MediaStream | null;
  remoteStream: MediaStream | null; // Keep for backward compatibility (1-on-1 calls)
  remoteStreams: Map<string, MediaStream>; // Map of socketId -> remote stream (for multiple participants)
  pendingIceCandidates: Map<string, RTCIceCandidateInit[]>; // Map of socketId -> Array of queued ICE candidates
  speechRecognition: any | null;
  callRecorder: MediaRecorder | null; // For recording the call
  recordingChunks: Blob[]; // Store recording chunks
  
  roomId: string | null;
  callId: string | null;
  userName: string | null; // Store user name for transcript
  isHost: boolean;
  callStatus: 'idle' | 'connecting' | 'waiting' | 'active' | 'ended';
  participants: Participant[];
  callStartTime: number | null; // Timestamp when call became active (for continuous duration)
  
  transcript: TranscriptSegment[];
  interimTranscript: string; // For showing live interim results
  aiNotes: AINotes | null;
  aiAnalysisInterval: ReturnType<typeof setInterval> | null; // For periodic AI analysis
  
  isMuted: boolean;
  isVideoOff: boolean;
  isRecording: boolean;
  isMinimized: boolean; // For floating PiP overlay when navigating away
  
  // Performance optimization states
  dominantSpeaker: string | null; // socketId of current dominant speaker
  connectionQuality: Map<string, 'good' | 'fair' | 'poor'>; // socketId -> quality
  reconnectionAttempts: Map<string, number>; // socketId -> retry count (max 3)
  videoQuality: 'default' | 'medium' | 'low'; // Current video quality setting
  monitoringIntervals: Map<string, ReturnType<typeof setInterval>>; // socketId -> interval ID
  
  error: string | null;
  
  initSocket: (token: string, userName: string) => void;
  disconnectSocket: () => void;
  createRoom: (token: string) => Promise<string>;
  joinRoom: (roomId: string, token: string) => Promise<void>;
  leaveRoom: () => void;
  endCall: () => void;
  toggleMute: () => void;
  toggleVideo: () => void;
  sendTranscript: (text: string) => void;
  requestNotes: () => void;
  startAIAnalysis: () => void;
  stopAIAnalysis: () => void;
  analyzeTranscript: (isFinal?: boolean) => Promise<void>;
  startSpeechRecognition: () => void;
  stopSpeechRecognition: () => void;
  startCallRecording: () => void;
  stopCallRecording: () => Promise<void>;
  clearCall: () => void;
  minimizeCall: () => void;
  maximizeCall: () => void;
}

// Network Resilience: Include TURN server for restrictive firewalls
const ICE_SERVERS: RTCConfiguration = {
  iceServers: [
    { urls: 'stun:stun.l.google.com:19302' },
    { urls: 'stun:stun1.l.google.com:19302' },
    {
      urls: 'turn:openrelay.metered.ca:80',
      username: 'openrelayproject',
      credential: 'openrelayproject',
    },
    {
      urls: 'turn:openrelay.metered.ca:443',
      username: 'openrelayproject',
      credential: 'openrelayproject',
    },
  ],
  iceCandidatePoolSize: 10, // Pre-gather candidates for faster connection
};

// Maximum participants for mesh topology (performance limitation)
const MAX_PARTICIPANTS = 8;

// Video quality presets based on participant count
const VIDEO_QUALITY_PRESETS = {
  default: { width: 1280, height: 720, frameRate: 30 }, // 1-3 users
  medium: { width: 640, height: 480, frameRate: 30 },   // 4-6 users
  low: { width: 480, height: 360, frameRate: 24 },      // 7+ users
};

export const useCallStore = create<CallState>((set, get) => {
  // Expose debug utilities to window object in development mode
  if (typeof window !== 'undefined' && (import.meta.env.DEV || import.meta.env.MODE === 'development')) {
    (window as any).__callStoreDebug = {
      getState: () => get(),
      getPeerConnections: () => {
        const state = get();
        const pcs: Record<string, any> = {};
        state.peerConnections.forEach((pc, socketId) => {
          const participant = state.participants.find(p => p.socketId === socketId);
          pcs[socketId] = {
            connectionState: pc.connectionState,
            iceConnectionState: pc.iceConnectionState,
            signalingState: pc.signalingState,
            iceGatheringState: pc.iceGatheringState,
            participant: participant?.userName || 'Unknown',
            stats: null, // Will be populated by getStats()
          };
        });
        return pcs;
      },
      getConnectionQuality: () => {
        const state = get();
        const quality: Record<string, any> = {};
        state.connectionQuality.forEach((q, socketId) => {
          const participant = state.participants.find(p => p.socketId === socketId);
          quality[socketId] = {
            quality: q,
            participant: participant?.userName || 'Unknown',
          };
        });
        return quality;
      },
      getStats: async (socketId?: string) => {
        const state = get();
        if (socketId) {
          const pc = state.peerConnections.get(socketId);
          if (pc) {
            const stats = await pc.getStats();
            const result: any = {};
            stats.forEach((report: any) => {
              if (!result[report.type]) result[report.type] = [];
              result[report.type].push(report);
            });
            return result;
          }
          return null;
        }
        // Get stats for all connections
        const allStats: Record<string, any> = {};
        for (const [id, pc] of state.peerConnections.entries()) {
          try {
            const stats = await pc.getStats();
            const result: any = {};
            stats.forEach((report: any) => {
              if (!result[report.type]) result[report.type] = [];
              result[report.type].push(report);
            });
            allStats[id] = result;
          } catch (error) {
            allStats[id] = { error: (error as Error).message };
          }
        }
        return allStats;
      },
      getRemoteStreams: () => {
        const state = get();
        const streams: Record<string, any> = {};
        state.remoteStreams.forEach((stream, socketId) => {
          const participant = state.participants.find(p => p.socketId === socketId);
          streams[socketId] = {
            id: stream.id,
            active: stream.active,
            videoTracks: stream.getVideoTracks().length,
            audioTracks: stream.getAudioTracks().length,
            participant: participant?.userName || 'Unknown',
          };
        });
        return streams;
      },
      getParticipants: () => get().participants,
      getDominantSpeaker: () => {
        const state = get();
        const speaker = state.dominantSpeaker;
        const participant = speaker ? state.participants.find(p => p.socketId === speaker) : null;
        return speaker ? {
          socketId: speaker,
          userName: participant?.userName || 'Unknown',
        } : null;
      },
      getVideoQuality: () => get().videoQuality,
      getReconnectionAttempts: () => {
        const state = get();
        const attempts: Record<string, any> = {};
        state.reconnectionAttempts.forEach((count, socketId) => {
          const participant = state.participants.find(p => p.socketId === socketId);
          attempts[socketId] = {
            attempts: count,
            participant: participant?.userName || 'Unknown',
          };
        });
        return attempts;
      },
      forceQualityChange: async (participantCount: number) => {
        const { localStream } = get();
        if (!localStream) {
          console.warn('[DEBUG] No local stream available');
          return;
        }
        const quality = participantCount >= 7 
          ? VIDEO_QUALITY_PRESETS.low 
          : participantCount >= 4 
          ? VIDEO_QUALITY_PRESETS.medium 
          : VIDEO_QUALITY_PRESETS.default;
        
        const videoTrack = localStream.getVideoTracks()[0];
        if (videoTrack) {
          try {
            await videoTrack.applyConstraints({
              width: { ideal: quality.width },
              height: { ideal: quality.height },
              frameRate: { ideal: quality.frameRate },
            });
            console.log(`[DEBUG] Forced quality change: ${quality.width}x${quality.height} @ ${quality.frameRate}fps`);
          } catch (error: any) {
            console.error('[DEBUG] Error forcing quality change:', error.message);
          }
        }
      },
      logSummary: () => {
        const state = get();
        console.group('üîç Call Store Debug Summary');
        console.log('Participants:', state.participants.length);
        console.log('Peer Connections:', state.peerConnections.size);
        console.log('Remote Streams:', state.remoteStreams.size);
        console.log('Video Quality:', state.videoQuality);
        console.log('Dominant Speaker:', state.dominantSpeaker ? state.participants.find(p => p.socketId === state.dominantSpeaker)?.userName : 'None');
        console.log('Connection Quality:', Object.fromEntries(state.connectionQuality));
        console.log('Reconnection Attempts:', Object.fromEntries(state.reconnectionAttempts));
        console.groupEnd();
      },
      help: () => {
        console.log(`
üîß Call Store Debug Utilities

Available commands:
  __callStoreDebug.getState()                    - Get full store state
  __callStoreDebug.getPeerConnections()          - Get peer connection status
  __callStoreDebug.getConnectionQuality()        - Get connection quality for each peer
  __callStoreDebug.getStats(socketId?)           - Get WebRTC stats (all or specific peer)
  __callStoreDebug.getRemoteStreams()            - Get remote stream information
  __callStoreDebug.getParticipants()             - Get participants list
  __callStoreDebug.getDominantSpeaker()          - Get current dominant speaker
  __callStoreDebug.getVideoQuality()             - Get current video quality setting
  __callStoreDebug.getReconnectionAttempts()     - Get reconnection attempt counts
  __callStoreDebug.forceQualityChange(count)     - Force video quality change (for testing)
  __callStoreDebug.logSummary()                  - Log summary of current state
  __callStoreDebug.help()                        - Show this help message

Examples:
  __callStoreDebug.logSummary()
  __callStoreDebug.getStats()
  __callStoreDebug.forceQualityChange(5)
        `);
      },
    };
    
    // Show help on first load
    console.log('üîß Call Store Debug utilities available! Type __callStoreDebug.help() for commands.');
  }

  return {
  socket: null,
  peerConnection: null,
  peerConnections: new Map<string, RTCPeerConnection>(),
  localStream: null,
  remoteStream: null,
  remoteStreams: new Map<string, MediaStream>(),
  pendingIceCandidates: new Map<string, RTCIceCandidateInit[]>(),
  speechRecognition: null,
  callRecorder: null,
  recordingChunks: [],
  
  roomId: null,
  callId: null,
  userName: null,
  isHost: false,
  callStatus: 'idle',
  participants: [],
  callStartTime: null,
  
  transcript: [],
  interimTranscript: '',
  aiNotes: null,
  aiAnalysisInterval: null,
  
  isMuted: false,
  isVideoOff: false,
  isRecording: false,
  isMinimized: false,
  
  // Performance optimization states
  dominantSpeaker: null,
  connectionQuality: new Map<string, 'good' | 'fair' | 'poor'>(),
  reconnectionAttempts: new Map<string, number>(),
  videoQuality: 'default',
  monitoringIntervals: new Map<string, ReturnType<typeof setInterval>>(),
  
  error: null,

  initSocket: (token: string, userName: string) => {
    const existingSocket = get().socket;
    if (existingSocket?.connected) {
      console.log('[SOCKET] Already connected, skipping initialization');
      return;
    }

    // Disconnect existing socket if it exists but not connected
    if (existingSocket) {
      console.log('[SOCKET] Disconnecting existing socket');
      existingSocket.disconnect();
    }

    // Store user name for transcript
    set({ userName });

    const socket = io(SOCKET_URL, {
      auth: { token, userName },
      transports: ['websocket', 'polling'], // Fallback to polling if websocket fails
      reconnection: true,
      reconnectionDelay: 1000,
      reconnectionAttempts: 10, // Increase attempts for better reliability
      timeout: 20000, // 20 second connection timeout
      upgrade: true, // Allow transport upgrades
      rememberUpgrade: true, // Remember successful transport
      forceNew: false, // Reuse existing connection if available
    });
    
    // Store socket immediately so joinRoom can access it
    set({ socket });
    
    console.log('[SOCKET] Initializing connection to:', SOCKET_URL);
    console.log('[SOCKET] Socket instance created, waiting for connection...');

    // Track connection attempts to avoid showing errors too early
    let connectionAttempts = 0;
    let errorToastShown = false;

    socket.on('connect', () => {
      console.log('[SOCKET] ‚úÖ Connected successfully');
      console.log('[SOCKET] Socket ID:', socket.id);
      console.log('[SOCKET] Transport:', socket.io.engine?.transport?.name || 'unknown');
      set({ error: null });
      // Reset error tracking on successful connection
      connectionAttempts = 0;
      errorToastShown = false;
      // Don't show toast on every connect (too noisy, connection is expected)
      // toast.success('Connected', 'Socket connection established');
    });
    
    socket.on('connect_error', (error: any) => {
      connectionAttempts++;
      
      // Log error for debugging
      console.warn('[SOCKET] ‚ö†Ô∏è Connection error (will retry):', error.message, `Attempt ${connectionAttempts}`);
      
      // Transport errors are expected - Socket.IO will automatically try polling
      const isTransportError = error.type === 'TransportError' || 
                               error.message === 'websocket error' ||
                               error.message?.includes('websocket') ||
                               error.message?.includes('transport');
      
      if (isTransportError) {
        // Transport error - just log, Socket.IO will automatically try polling
        console.log('[SOCKET] WebSocket transport failed, will try polling fallback...');
        return; // Don't show error for transport failures
      }
      
      // For non-transport errors, only show error after multiple failed attempts
      // This gives Socket.IO time to retry and potentially succeed
      if (connectionAttempts >= 5 && !errorToastShown) {
        console.error('[SOCKET] ‚ùå Connection failed after multiple attempts:', {
          message: error.message,
          type: error.type || 'unknown',
          attempts: connectionAttempts,
        });
        
        errorToastShown = true;
        const message = `Unable to connect to call server: ${error.message || 'Connection failed'}`;
        set({ error: message });
        toast.error('Connection Error', message);
      }
    });
    
    // Reset error tracking on successful connection
    socket.on('connect', () => {
      connectionAttempts = 0;
      errorToastShown = false;
    });
    
    socket.on('reconnect', (attemptNumber) => {
      console.log('[SOCKET] ‚úÖ Reconnected after', attemptNumber, 'attempts');
      // Only show toast if it took multiple attempts (significant reconnection)
      if (attemptNumber > 3) {
        toast.success('Reconnected', 'Connection restored');
      }
      
      // Rejoin room if we were in one
      const { roomId } = get();
      if (roomId && socket.connected) {
        console.log('[SOCKET] Rejoining room after reconnect:', roomId);
        socket.emit('room:join', { roomId });
      }
    });
    
    socket.on('reconnect_error', (error) => {
      // Only log, don't show error - Socket.IO will keep trying
      console.warn('[SOCKET] ‚ö†Ô∏è Reconnection attempt failed (will retry):', error.message || error);
    });
    
    socket.on('reconnect_failed', () => {
      console.error('[SOCKET] ‚ùå Reconnection failed - giving up');
      toast.error('Connection Failed', 'Unable to reconnect. Please refresh the page.');
    });

    socket.on('disconnect', (reason) => {
      console.log('[SOCKET] Disconnected:', reason);
      // Only show warning for unexpected disconnects, not for manual disconnects
      if (reason === 'io server disconnect' || reason === 'transport close') {
        console.log('[SOCKET] Unexpected disconnect, will attempt to reconnect...');
        // Don't show toast for normal reconnection scenarios
      } else if (reason === 'io client disconnect') {
        console.log('[SOCKET] Client-initiated disconnect');
      }
    });

    socket.on('error', (error: any) => {
      console.error('Socket error:', error);
      const message = error.message || 'An error occurred';
      set({ error: message });
      toast.error('Error', message);
    });

    // ========== PERFORMANCE OPTIMIZATION HELPER FUNCTIONS ==========
    
    /**
     * Get adaptive video quality based on participant count
     */
    const getAdaptiveVideoQuality = (participantCount: number): { width: number; height: number; frameRate: number } => {
      if (participantCount >= 7) {
        return VIDEO_QUALITY_PRESETS.low;
      } else if (participantCount >= 4) {
        return VIDEO_QUALITY_PRESETS.medium;
      }
      return VIDEO_QUALITY_PRESETS.default;
    };

    /**
     * Apply adaptive video quality to local stream
     */
    const applyAdaptiveVideoQuality = async (participantCount: number): Promise<void> => {
      const { localStream } = get();
      if (!localStream) return;

      const quality = getAdaptiveVideoQuality(participantCount + 1); // +1 for self
      const videoTrack = localStream.getVideoTracks()[0];
      
      if (!videoTrack) return;

      try {
        await videoTrack.applyConstraints({
          width: { ideal: quality.width },
          height: { ideal: quality.height },
          frameRate: { ideal: quality.frameRate },
        });
        console.log(`[OPTIMIZATION] Applied video quality: ${quality.width}x${quality.height} @ ${quality.frameRate}fps (${participantCount + 1} participants)`);
        
        // Update quality state
        const qualityLevel = participantCount >= 7 ? 'low' : participantCount >= 4 ? 'medium' : 'default';
        set({ videoQuality: qualityLevel });
      } catch (error: any) {
        console.error('[OPTIMIZATION] Error applying video constraints:', error.message);
      }
    };

    /**
     * Monitor connection quality using getStats() API
     */
    const monitorConnectionQuality = (socketId: string, pc: RTCPeerConnection): void => {
      const intervalId = setInterval(async () => {
        try {
          const stats = await pc.getStats();
          let packetsLost = 0;
          let packetsReceived = 0;
          let jitter = 0;

          stats.forEach((report: any) => {
            if (report.type === 'inbound-rtp' && report.mediaType === 'video') {
              packetsLost += report.packetsLost || 0;
              packetsReceived += report.packetsReceived || 0;
              jitter += report.jitter || 0;
            }
            if (report.type === 'inbound-rtp' && report.mediaType === 'audio') {
              packetsLost += report.packetsLost || 0;
              packetsReceived += report.packetsReceived || 0;
              jitter += report.jitter || 0;
            }
          });

          const totalPackets = packetsLost + packetsReceived;
          const packetLossRate = totalPackets > 0 ? packetsLost / totalPackets : 0;
          
          // Determine quality based on packet loss and jitter
          let quality: 'good' | 'fair' | 'poor' = 'good';
          if (packetLossRate > 0.05 || jitter > 50) {
            quality = 'poor';
          } else if (packetLossRate > 0.02 || jitter > 20) {
            quality = 'fair';
          }

          const { connectionQuality } = get();
          const updatedQuality = new Map(connectionQuality);
          updatedQuality.set(socketId, quality);
          set({ connectionQuality: updatedQuality });

          // Auto-reduce quality for poor connections
          if (quality === 'poor') {
            const { remoteStreams } = get();
            const remoteStream = remoteStreams.get(socketId);
            if (remoteStream) {
              const videoTrack = remoteStream.getVideoTracks()[0];
              if (videoTrack) {
                try {
                  await videoTrack.applyConstraints({
                    width: { ideal: 480 },
                    height: { ideal: 360 },
                    frameRate: { ideal: 20 },
                  });
                  console.log(`[OPTIMIZATION] Reduced quality for ${socketId} due to poor connection`);
                } catch (error: any) {
                  console.error('[OPTIMIZATION] Error reducing quality:', error.message);
                }
              }
            }
          }
        } catch (error: any) {
          console.error(`[OPTIMIZATION] Error monitoring connection quality for ${socketId}:`, error.message);
        }
      }, 5000); // Check every 5 seconds

      // Store interval ID for cleanup
      const { monitoringIntervals } = get();
      const updatedIntervals = new Map(monitoringIntervals);
      updatedIntervals.set(socketId, intervalId);
      set({ monitoringIntervals: updatedIntervals });
    };

    /**
     * Detect dominant speaker using Web Audio API
     */
    const setupDominantSpeakerDetection = (socketId: string, stream: MediaStream): void => {
      try {
        const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
        const analyser = audioContext.createAnalyser();
        analyser.fftSize = 256;
        const dataArray = new Uint8Array(analyser.frequencyBinCount);
        
        const source = audioContext.createMediaStreamSource(stream);
        source.connect(analyser);

        const checkAudioLevel = () => {
          analyser.getByteFrequencyData(dataArray);
          const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
          
          // Threshold for "speaking" (adjust based on testing)
          if (average > 30) {
            const { dominantSpeaker } = get();
            if (dominantSpeaker !== socketId) {
              set({ dominantSpeaker: socketId });
              console.log(`[OPTIMIZATION] Dominant speaker changed to ${socketId}`);
            }
          }
        };

        const intervalId = setInterval(checkAudioLevel, 200); // Check every 200ms
        
        // Clean up when stream ends
        stream.getAudioTracks()[0]?.addEventListener('ended', () => {
          clearInterval(intervalId);
          audioContext.close();
        });
      } catch (error: any) {
        console.error(`[OPTIMIZATION] Error setting up dominant speaker detection for ${socketId}:`, error.message);
      }
    };


    // ========== MESH TOPOLOGY HELPER FUNCTIONS ==========
    
    /**
     * Create a peer connection for a specific remote socket (mesh topology)
     */
    const createPeerConnection = (remoteSocketId: string): RTCPeerConnection => {
      const { peerConnections, localStream, socket: currentSocket } = get();
      
      // Don't create duplicate connections
      if (peerConnections.has(remoteSocketId)) {
        console.log(`[MESH] Peer connection already exists for ${remoteSocketId}`);
        return peerConnections.get(remoteSocketId)!;
      }

      console.log(`[MESH] Creating new peer connection for ${remoteSocketId}`);
      
      // Create new peer connection
      const pc = new RTCPeerConnection(ICE_SERVERS);

      // Add local stream tracks
      if (localStream) {
        localStream.getTracks().forEach(track => {
          pc.addTrack(track, localStream);
          console.log(`[MESH] Added ${track.kind} track to peer connection for ${remoteSocketId}`);
        });
      }

      // Handle ICE candidates
      pc.onicecandidate = (event) => {
        if (event.candidate && currentSocket && currentSocket.connected) {
          console.log(`[MESH] Sending ICE candidate from ${socket.id} to ${remoteSocketId}`);
          currentSocket.emit('ice-candidate', {
            to: remoteSocketId,
            candidate: event.candidate,
            from: socket.id,
          });
        }
      };

      // Handle remote stream
      pc.ontrack = (event) => {
        console.log(`[MESH] Received remote track from ${remoteSocketId}`);
        if (event.streams && event.streams[0]) {
          const { remoteStreams } = get();
          const updatedRemoteStreams = new Map(remoteStreams);
          updatedRemoteStreams.set(remoteSocketId, event.streams[0]);
          set({ remoteStreams: updatedRemoteStreams });
          
          // Set up dominant speaker detection for audio tracks
          const audioTracks = event.streams[0].getAudioTracks();
          if (audioTracks.length > 0) {
            setupDominantSpeakerDetection(remoteSocketId, event.streams[0]);
          }
          
          // Start connection quality monitoring
          monitorConnectionQuality(remoteSocketId, pc);
          
          // Also update participants list to trigger UI update
          const { participants } = get();
          const participant = participants.find(p => p.socketId === remoteSocketId);
          if (!participant) {
            console.warn(`[MESH] Received track from unknown participant: ${remoteSocketId}`);
          }
        }
      };

      // Handle connection state changes with error recovery
      pc.onconnectionstatechange = () => {
        console.log(`[MESH] Peer connection state with ${remoteSocketId}: ${pc.connectionState}`);
        if (pc.connectionState === 'connected') {
          console.log(`[MESH] ‚úÖ Connected to ${remoteSocketId}`);
          // Reset reconnection attempts on successful connection
          const { reconnectionAttempts } = get();
          const updatedAttempts = new Map(reconnectionAttempts);
          updatedAttempts.delete(remoteSocketId);
          set({ reconnectionAttempts: updatedAttempts });
          
          // Start connection quality monitoring
          monitorConnectionQuality(remoteSocketId, pc);
        } else if (pc.connectionState === 'failed') {
          console.warn(`[MESH] ‚ö†Ô∏è Connection failed with ${remoteSocketId}, attempting reconnection...`);
          // Attempt automatic reconnection (using the version defined after createPeerConnection)
          attemptReconnectionAfter(remoteSocketId);
        } else if (pc.connectionState === 'disconnected') {
          console.warn(`[MESH] ‚ö†Ô∏è Connection disconnected with ${remoteSocketId}`);
          // Stop monitoring
          const { monitoringIntervals } = get();
          const intervalId = monitoringIntervals.get(remoteSocketId);
          if (intervalId) {
            clearInterval(intervalId);
            const updatedIntervals = new Map(monitoringIntervals);
            updatedIntervals.delete(remoteSocketId);
            set({ monitoringIntervals: updatedIntervals });
          }
        }
      };

      // Store peer connection
      const updatedPeerConnections = new Map(peerConnections);
      updatedPeerConnections.set(remoteSocketId, pc);
      set({ peerConnections: updatedPeerConnections });

      // Initialize pending ICE candidates queue
      const { pendingIceCandidates } = get();
      const updatedPendingCandidates = new Map(pendingIceCandidates);
      if (!updatedPendingCandidates.has(remoteSocketId)) {
        updatedPendingCandidates.set(remoteSocketId, []);
      }
      set({ pendingIceCandidates: updatedPendingCandidates });

      return pc;
    };

    /**
     * Process queued ICE candidates for a peer connection
     */
    const processQueuedIceCandidates = async (socketId: string): Promise<void> => {
      const { peerConnections, pendingIceCandidates } = get();
      const pc = peerConnections.get(socketId);
      if (!pc) {
        console.warn(`[MESH] Cannot process queued ICE candidates - no peer connection for ${socketId}`);
        return;
      }

      const queued = pendingIceCandidates.get(socketId) || [];
      if (queued.length === 0) {
        return;
      }

      console.log(`[MESH] Processing ${queued.length} queued ICE candidates for ${socketId}`);

      for (const candidate of queued) {
        try {
          await pc.addIceCandidate(new RTCIceCandidate(candidate));
          console.log(`[MESH] ‚úÖ Added queued ICE candidate for ${socketId}`);
        } catch (error: any) {
          console.error(`[MESH] ‚ùå Error adding queued ICE candidate for ${socketId}:`, error.message);
        }
      }

      // Clear queue
      const updatedPendingCandidates = new Map(pendingIceCandidates);
      updatedPendingCandidates.set(socketId, []);
      set({ pendingIceCandidates: updatedPendingCandidates });
    };

    /**
     * Attempt to reconnect a failed peer connection (moved here to avoid circular dependency)
     */
    const attemptReconnectionAfter = async (socketId: string): Promise<void> => {
      const { reconnectionAttempts, socket: currentSocket } = get();
      const attempts = reconnectionAttempts.get(socketId) || 0;
      
      if (attempts >= 3) {
        const { participants } = get();
        const participantName = participants.find((p: Participant) => p.socketId === socketId)?.userName || 'participant';
        console.error(`[RECOVERY] Max reconnection attempts reached for ${socketId}`);
        toast.error('Connection Failed', `Unable to reconnect to ${participantName}`);
        return;
      }

      console.log(`[RECOVERY] Attempting reconnection ${attempts + 1}/3 for ${socketId}`);
      
      const updatedAttempts = new Map(reconnectionAttempts);
      updatedAttempts.set(socketId, attempts + 1);
      set({ reconnectionAttempts: updatedAttempts });

      try {
        // Close failed connection
        const { peerConnections } = get();
        const oldPc = peerConnections.get(socketId);
        if (oldPc) {
          oldPc.close();
        }

        // Remove from Maps
        const updatedPeerConnections = new Map(peerConnections);
        updatedPeerConnections.delete(socketId);
        set({ peerConnections: updatedPeerConnections });

        // Wait a bit before retrying
        await new Promise(resolve => setTimeout(resolve, 2000));

        // Create new peer connection using the helper function (now it's defined)
        const newPc = createPeerConnection(socketId);

        // Send new offer
        const offer = await newPc.createOffer({
          offerToReceiveAudio: true,
          offerToReceiveVideo: true,
        });

        await newPc.setLocalDescription(offer);

        if (currentSocket && currentSocket.connected) {
          currentSocket.emit('offer', {
            to: socketId,
            offer: newPc.localDescription,
            from: currentSocket.id,
          });
          console.log(`[RECOVERY] Sent reconnection offer to ${socketId}`);
        }
      } catch (error: any) {
        console.error(`[RECOVERY] Error during reconnection attempt for ${socketId}:`, error.message);
      }
    };

    // ========== MESH TOPOLOGY SOCKET EVENT HANDLERS ==========

    /**
     * Handle 'room-users' event - existing users when joining (MESH TOPOLOGY)
     * Joining user receives list of existing users and creates offers for each
     */
    socket.on('room:users', async (data: { users: Array<{ socketId: string; userId: string; userName: string }> }) => {
      console.log(`[MESH] Received room-users event with ${data.users.length} existing users`);
      
      const { localStream } = get();
      if (!localStream) {
        console.error('[MESH] Cannot create peer connections - no local stream');
        return;
      }

      // Check participant limit (Memory Management)
      const totalParticipants = data.users.length + 1; // +1 for self
      if (totalParticipants > MAX_PARTICIPANTS) {
        toast.warning('Participant Limit', `Maximum ${MAX_PARTICIPANTS} participants reached. Consider using breakout rooms.`);
        console.warn(`[OPTIMIZATION] Participant limit exceeded: ${totalParticipants}/${MAX_PARTICIPANTS}`);
        return;
      }

      // Apply adaptive video quality based on participant count
      await applyAdaptiveVideoQuality(data.users.length);

      // For each existing user, create peer connection and send offer
      for (const user of data.users) {
        console.log(`[MESH] Creating peer connection and sending offer to ${user.socketId} (${user.userName})`);
        
        try {
          const pc = createPeerConnection(user.socketId);
          
          // Create offer
          const offer = await pc.createOffer({
            offerToReceiveAudio: true,
            offerToReceiveVideo: true,
          });
          
          // Set local description
          await pc.setLocalDescription(offer);
          
          // Send offer
          socket.emit('offer', {
            to: user.socketId,
            offer: pc.localDescription,
            from: socket.id,
          });
          
          console.log(`[MESH] ‚úÖ Sent offer to ${user.socketId}`);
        } catch (error: any) {
          console.error(`[MESH] ‚ùå Error creating offer for ${user.socketId}:`, error.message);
        }
      }
    });

    /**
     * Handle 'offer' event - receive offer from another user (MESH TOPOLOGY)
     */
    socket.on('offer', async (data: { from: string; offer: RTCSessionDescriptionInit }) => {
      const { from: senderSocketId, offer: sdpOffer } = data;
      console.log(`[MESH] Received offer from ${senderSocketId}`);

      const { localStream } = get();
      if (!localStream) {
        console.error('[MESH] Cannot handle offer - no local stream');
        return;
      }

      try {
        // Get or create peer connection
        const pc = createPeerConnection(senderSocketId);
        
        // Set remote description
        await pc.setRemoteDescription(new RTCSessionDescription(sdpOffer));
        console.log(`[MESH] ‚úÖ Set remote description from ${senderSocketId}`);
        
        // Process any queued ICE candidates
        await processQueuedIceCandidates(senderSocketId);
        
        // Create answer
        const answer = await pc.createAnswer({
          offerToReceiveAudio: true,
          offerToReceiveVideo: true,
        });
        
        // Set local description
        await pc.setLocalDescription(answer);
        
        // Send answer
        socket.emit('answer', {
          to: senderSocketId,
          answer: pc.localDescription,
          from: socket.id,
        });
        
        console.log(`[MESH] ‚úÖ Sent answer to ${senderSocketId}`);
      } catch (error: any) {
        console.error(`[MESH] ‚ùå Error handling offer from ${senderSocketId}:`, error.message);
      }
    });

    /**
     * Handle 'answer' event - receive answer to our offer (MESH TOPOLOGY)
     */
    socket.on('answer', async (data: { from: string; answer: RTCSessionDescriptionInit }) => {
      const { from: senderSocketId, answer: sdpAnswer } = data;
      console.log(`[MESH] Received answer from ${senderSocketId}`);

      const { peerConnections } = get();
      const pc = peerConnections.get(senderSocketId);
      
      if (!pc) {
        console.error(`[MESH] Cannot handle answer - no peer connection for ${senderSocketId}`);
        return;
      }

      try {
        // Set remote description
        await pc.setRemoteDescription(new RTCSessionDescription(sdpAnswer));
        console.log(`[MESH] ‚úÖ Set remote answer from ${senderSocketId}`);
        
        // Process any queued ICE candidates
        await processQueuedIceCandidates(senderSocketId);
        
        console.log(`[MESH] ‚úÖ Connection established with ${senderSocketId}`);
      } catch (error: any) {
        console.error(`[MESH] ‚ùå Error handling answer from ${senderSocketId}:`, error.message);
      }
    });

    /**
     * Handle 'ice-candidate' event - receive ICE candidate (MESH TOPOLOGY)
     */
    socket.on('ice-candidate', async (data: { from: string; candidate: RTCIceCandidateInit | null }) => {
      const { from: senderSocketId, candidate } = data;
      
      if (!candidate) {
        console.log(`[MESH] Received null ICE candidate (end of candidates) from ${senderSocketId}`);
        return;
      }

      const { peerConnections } = get();
      const pc = peerConnections.get(senderSocketId);
      
      if (!pc) {
        console.warn(`[MESH] Cannot add ICE candidate - no peer connection for ${senderSocketId}, queuing`);
        // Queue candidate for later
        const { pendingIceCandidates } = get();
        const updatedPendingCandidates = new Map(pendingIceCandidates);
        const queue = updatedPendingCandidates.get(senderSocketId) || [];
        queue.push(candidate);
        updatedPendingCandidates.set(senderSocketId, queue);
        set({ pendingIceCandidates: updatedPendingCandidates });
        return;
      }

      try {
        // Check if remote description is set
        if (pc.remoteDescription) {
          // Add candidate immediately
          await pc.addIceCandidate(new RTCIceCandidate(candidate));
          console.log(`[MESH] ‚úÖ Added ICE candidate from ${senderSocketId}`);
        } else {
          // Queue candidate
          console.log(`[MESH] Remote description not set yet, queuing ICE candidate from ${senderSocketId}`);
          const { pendingIceCandidates } = get();
          const updatedPendingCandidates = new Map(pendingIceCandidates);
          const queue = updatedPendingCandidates.get(senderSocketId) || [];
          queue.push(candidate);
          updatedPendingCandidates.set(senderSocketId, queue);
          set({ pendingIceCandidates: updatedPendingCandidates });
        }
      } catch (error: any) {
        if (error.message?.includes('duplicate') || error.message?.includes('Invalid')) {
          console.log(`[MESH] ‚ÑπÔ∏è Ignoring duplicate/invalid ICE candidate from ${senderSocketId}`);
        } else {
          console.error(`[MESH] ‚ùå Error adding ICE candidate from ${senderSocketId}:`, error.message);
        }
      }
    });

    /**
     * Handle 'user-left' event - user disconnected (MESH TOPOLOGY)
     */
    socket.on('user-left', (data: { socketId: string; userId?: string }) => {
      const { socketId: leavingSocketId } = data;
      console.log(`[MESH] User ${leavingSocketId} left, cleaning up connection`);

      const { peerConnections, remoteStreams, participants } = get();
      
      // Close and remove peer connection
      const pc = peerConnections.get(leavingSocketId);
      if (pc) {
        pc.close();
        console.log(`[MESH] Closed peer connection to ${leavingSocketId}`);
      }
      
      const updatedPeerConnections = new Map(peerConnections);
      updatedPeerConnections.delete(leavingSocketId);
      
      // Remove remote stream
      const updatedRemoteStreams = new Map(remoteStreams);
      updatedRemoteStreams.delete(leavingSocketId);
      
      // Remove from participants list
      const updatedParticipants = participants.filter(p => p.socketId !== leavingSocketId);
      
      // Clean up pending ICE candidates
      const { pendingIceCandidates } = get();
      const updatedPendingCandidates = new Map(pendingIceCandidates);
      updatedPendingCandidates.delete(leavingSocketId);
      
      set({
        peerConnections: updatedPeerConnections,
        remoteStreams: updatedRemoteStreams,
        participants: updatedParticipants,
        pendingIceCandidates: updatedPendingCandidates,
      });
    });

    // ========== END MESH TOPOLOGY HANDLERS ==========

    socket.on('room:joined', (data) => {
      // Filter out current user from participants list and remove duplicates
      const authUser = useAuthStore.getState().user;
      const currentUserId = authUser?._id;
      const currentSocketId = socket.id;
      
      // CRITICAL: Deduplicate participants by userId (UI-level deduplication)
      // This ensures no duplicate participants even if backend sends duplicates
      const participantMap = new Map<string, any>();
      (data.participants || []).forEach((p: any) => {
        // Filter out current user
        if (p.userId === currentUserId || p.socketId === currentSocketId) {
          return;
        }
        // Use userId as key for deduplication (most reliable identifier)
        const userId = p.userId || p._id;
        if (userId && !participantMap.has(userId)) {
          participantMap.set(userId, p);
        } else if (userId && participantMap.has(userId)) {
          // If duplicate found, keep the one with more complete data
          const existing = participantMap.get(userId);
          if (p.userName && !existing.userName) {
            participantMap.set(userId, p);
          }
        }
      });
      const filteredParticipants = Array.from(participantMap.values());
      
      console.log('[ROOM] Joined room with participants:', filteredParticipants.length);
      set({
        roomId: data.roomId,
        callId: data.callId,
        participants: filteredParticipants,
        callStatus: data.callStarted ? 'active' : 'waiting',
      });
    });

    socket.on('user:joined', async (data) => {
      // Don't add current user to participants list
      const currentSocketId = socket.id;
      if (data.socketId === currentSocketId) {
        console.log('[ROOM] Ignoring own join event');
        return;
      }
      
      console.log(`[MESH] üéâ New user joined: ${data.socketId} (${data.userName}), waiting for their offer`);
      
      set((state) => {
        // CRITICAL: Deduplicate by userId (most reliable identifier)
        const userId = data.userId || data._id;
        const exists = state.participants.some(p => 
          (p.userId === userId) || (p.socketId === data.socketId)
        );
        if (exists) {
          console.log('[ROOM] ‚ö†Ô∏è Participant already exists:', { userId, socketId: data.socketId });
          // Update existing participant if new data is more complete
          return {
            participants: state.participants.map(p => 
              (p.userId === userId || p.socketId === data.socketId) 
                ? { ...p, ...data } // Merge new data
                : p
            ),
          };
        }
        console.log('[ROOM] ‚úÖ Adding new participant:', data.userName, data.socketId);
        return {
          participants: [...state.participants, data],
        };
      });
      
      // MESH TOPOLOGY: Create peer connection but DON'T create offer
      // The new user will send us an offer, we'll respond with an answer
      const { peerConnection, localStream } = get();
      if (localStream) {
        console.log(`[MESH] Preparing to receive offer from ${data.socketId}`);
        // Peer connection will be created when we receive their offer
      }
      console.log('[WEBRTC] üîç Checking peer connection and local stream:', {
        hasPeerConnection: !!peerConnection,
        hasLocalStream: !!localStream,
        peerConnectionState: peerConnection ? {
          connectionState: peerConnection.connectionState,
          iceConnectionState: peerConnection.iceConnectionState,
          signalingState: peerConnection.signalingState,
        } : null,
      });
      
      if (peerConnection && localStream) {
        try {
          // CRITICAL: Verify tracks are added before creating offer
          const senders = peerConnection.getSenders();
          const hasVideoSender = senders.some(s => s.track && s.track.kind === 'video');
          const hasAudioSender = senders.some(s => s.track && s.track.kind === 'audio');
          
          console.log('[WEBRTC] üìä ========== PEER CONNECTION SENDERS CHECK ==========');
          console.log('[WEBRTC] üìä Sender details:', {
            totalSenders: senders.length,
            hasVideoSender,
            hasAudioSender,
            senders: senders.map(s => ({
              kind: s.track?.kind,
              enabled: s.track?.enabled,
              id: s.track?.id,
              readyState: s.track?.readyState,
            })),
          });
          
          // CRITICAL: If tracks are missing, add them now
          if (!hasVideoSender || !hasAudioSender) {
            console.warn('[WEBRTC] ‚ö†Ô∏è Missing tracks in peer connection, adding now...');
            localStream.getTracks().forEach(track => {
              const existingSender = senders.find(s => s.track && s.track.id === track.id);
              if (!existingSender) {
                console.log('[WEBRTC] ‚ûï Adding missing track:', {
                  kind: track.kind,
                  id: track.id,
                  enabled: track.enabled,
                  readyState: track.readyState,
                });
                try {
                  const sender = peerConnection.addTrack(track, localStream);
                  console.log('[WEBRTC] ‚úÖ Track added successfully:', {
                    kind: track.kind,
                    senderCreated: !!sender,
                  });
                } catch (error: any) {
                  console.error('[WEBRTC] ‚ùå Error adding track:', {
                    kind: track.kind,
                    error: error.message,
                  });
                }
              } else {
                console.log('[WEBRTC] ‚ÑπÔ∏è Track already added:', track.kind, track.id);
              }
            });
            
            // Re-check senders after adding
            const updatedSenders = peerConnection.getSenders();
            console.log('[WEBRTC] üìä Updated senders after adding tracks:', {
              totalSenders: updatedSenders.length,
              senders: updatedSenders.map(s => ({
                kind: s.track?.kind,
                enabled: s.track?.enabled,
              })),
            });
          }
          
          // Both host and non-host can create offers for 1-on-1 calls
          // For group calls, only host creates offers
          const isHost = get().isHost;
          const participantCount = get().participants.length;
          
          // Create offer if host OR if it's a 1-on-1 call (participantCount === 1 means this is the second person)
          if (isHost || participantCount === 1) {
            console.log('[WEBRTC] üéØ ========== CREATING OFFER ==========');
            console.log('[WEBRTC] üéØ Offer details:', {
              socketId: data.socketId,
              userName: data.userName,
              isHost,
              participantCount,
            });
            console.log('[WEBRTC] üìã Local stream tracks before offer:', {
              videoTracks: localStream.getVideoTracks().length,
              audioTracks: localStream.getAudioTracks().length,
              videoEnabled: localStream.getVideoTracks()[0]?.enabled,
              audioEnabled: localStream.getAudioTracks()[0]?.enabled,
              videoTrackId: localStream.getVideoTracks()[0]?.id,
              audioTrackId: localStream.getAudioTracks()[0]?.id,
            });
            
            const offer = await peerConnection.createOffer({
              offerToReceiveAudio: true,
              offerToReceiveVideo: true,
            });
            
            console.log('[WEBRTC] üì§ Offer created:', {
              type: offer.type,
              sdpLength: offer.sdp?.length || 0,
              sdpPreview: offer.sdp?.substring(0, 300) + '...',
              hasVideo: offer.sdp?.includes('m=video'),
              hasAudio: offer.sdp?.includes('m=audio'),
            });
            
            await peerConnection.setLocalDescription(offer);
            console.log('[WEBRTC] ‚úÖ Local description set:', {
              signalingState: peerConnection.signalingState,
              localDescriptionType: peerConnection.localDescription?.type,
            });
            
            socket.emit('signal:offer', {
              targetId: data.socketId,
              offer: peerConnection.localDescription,
            });
            console.log('[WEBRTC] üì° ========== OFFER SENT ==========');
            console.log('[WEBRTC] üì° Sent to:', data.socketId, data.userName);
            
            // Send any queued ICE candidates immediately
            // Note: ICE candidates are sent automatically by onicecandidate handler
            // But we should also send any that were queued before this participant joined
          } else {
            console.log('[WEBRTC] ‚è∏Ô∏è Not creating offer (not host and not 1-on-1):', {
              isHost,
              participantCount,
            });
          }
        } catch (error: any) {
          console.error('[WEBRTC] ‚ùå ========== ERROR CREATING OFFER ==========');
          console.error('[WEBRTC] ‚ùå Error details:', {
            message: error.message,
            stack: error.stack,
            name: error.name,
          });
          toast.error('Connection Error', 'Failed to establish video connection');
        }
      } else {
        console.error('[WEBRTC] ‚ùå ========== CANNOT CREATE OFFER ==========');
        console.error('[WEBRTC] ‚ùå Missing requirements:', {
          hasPeerConnection: !!peerConnection,
          hasLocalStream: !!localStream,
          peerConnectionState: peerConnection ? peerConnection.signalingState : 'N/A',
        });
      }
    });

    socket.on('user:left', (data) => {
      console.log('[CALL] User left:', data.userName, 'Remaining participants:', data.participantCount);
      set((state) => {
        const updatedParticipants = state.participants.filter(p => p.socketId !== data.socketId);
        console.log('[CALL] Updated participants count:', updatedParticipants.length);
        return {
          participants: updatedParticipants,
        };
      });
      
      // Keep call active - don't change call status
      // Call only ends when call:ended event is received (last participant left)
      const { callStatus } = get();
      if (callStatus === 'active') {
        console.log('[CALL] Call continues with remaining participants');
      }
    });

    socket.on('signal:offer', async (data) => {
      const { peerConnection, localStream } = get();
      if (!peerConnection) {
        console.error('[WEBRTC] ‚ùå Cannot handle offer - no peer connection');
        return;
      }
      
      try {
        console.log('[WEBRTC] üì• ========== RECEIVED OFFER ==========');
        console.log('[WEBRTC] üì• Offer from:', data.fromId);
        console.log('[WEBRTC] üì• Offer details:', {
          type: data.offer?.type,
          sdp: data.offer?.sdp?.substring(0, 200) + '...',
        });
        console.log('[WEBRTC] üì• Current signaling state:', peerConnection.signalingState);
        
        // CRITICAL: Ensure tracks are added BEFORE setting remote description
        if (localStream) {
          const senders = peerConnection.getSenders();
          const hasVideoSender = senders.some(s => s.track && s.track.kind === 'video');
          const hasAudioSender = senders.some(s => s.track && s.track.kind === 'audio');
          
          console.log('[WEBRTC] üìä Current senders before handling offer:', {
            totalSenders: senders.length,
            hasVideoSender,
            hasAudioSender,
          });
          
          if (!hasVideoSender || !hasAudioSender) {
            console.log('[WEBRTC] üîß Adding tracks before handling offer...');
            localStream.getTracks().forEach(track => {
              const existingSender = senders.find(s => s.track && s.track.id === track.id);
              if (!existingSender) {
                console.log('[WEBRTC] ‚ûï Adding track:', track.kind, track.id);
                try {
                  peerConnection.addTrack(track, localStream);
                  console.log('[WEBRTC] ‚úÖ Track added successfully');
                } catch (error: any) {
                  console.error('[WEBRTC] ‚ùå Error adding track:', error.message);
                }
              } else {
                console.log('[WEBRTC] ‚ÑπÔ∏è Track already added:', track.kind, track.id);
              }
            });
          }
        } else {
          console.warn('[WEBRTC] ‚ö†Ô∏è No local stream available when handling offer');
        }
        
        // CRITICAL: Set remote description first
        console.log('[WEBRTC] üîß Setting remote description...');
        await peerConnection.setRemoteDescription(new RTCSessionDescription(data.offer));
        console.log('[WEBRTC] ‚úÖ Remote description set, signaling state:', peerConnection.signalingState);
        
        // CRITICAL: Create answer with proper options
        console.log('[WEBRTC] üîß Creating answer...');
        const answer = await peerConnection.createAnswer({
          offerToReceiveAudio: true,
          offerToReceiveVideo: true,
        });
        
        console.log('[WEBRTC] üì§ Answer created:', {
          type: answer.type,
          sdp: answer.sdp?.substring(0, 200) + '...',
        });
        
        // CRITICAL: Set local description
        await peerConnection.setLocalDescription(answer);
        console.log('[WEBRTC] ‚úÖ Local description set, signaling state:', peerConnection.signalingState);
        
        // CRITICAL: Send answer
        socket.emit('signal:answer', {
          targetId: data.fromId,
          answer: peerConnection.localDescription,
        });
        console.log('[WEBRTC] üì° Answer sent via Socket.IO to:', data.fromId);
        console.log('[WEBRTC] üì• ========== OFFER HANDLED ==========');
      } catch (error: any) {
        console.error('[WEBRTC] ‚ùå Error handling offer:', error.message, error);
        toast.error('Connection Error', 'Failed to accept video connection');
      }
    });

    socket.on('signal:answer', async (data) => {
      const { peerConnection } = get();
      if (!peerConnection) {
        console.error('[WEBRTC] ‚ùå ========== CANNOT HANDLE ANSWER ==========');
        console.error('[WEBRTC] ‚ùå No peer connection available');
        return;
      }
      
      try {
        console.log('[WEBRTC] üì• ========== RECEIVED ANSWER ==========');
        console.log('[WEBRTC] üì• Answer details:', {
          fromId: data.fromId || 'unknown',
          answerType: data.answer?.type,
          sdpLength: data.answer?.sdp?.length || 0,
          sdpPreview: data.answer?.sdp?.substring(0, 300) + '...',
          hasVideo: data.answer?.sdp?.includes('m=video'),
          hasAudio: data.answer?.sdp?.includes('m=audio'),
        });
        console.log('[WEBRTC] üì• Current peer connection state:', {
          signalingState: peerConnection.signalingState,
          connectionState: peerConnection.connectionState,
          iceConnectionState: peerConnection.iceConnectionState,
          iceGatheringState: peerConnection.iceGatheringState,
        });
        
        // Check if we're in the right state
        if (peerConnection.signalingState === 'have-local-offer' || peerConnection.signalingState === 'stable') {
          console.log('[WEBRTC] üîß Setting remote answer...');
          await peerConnection.setRemoteDescription(new RTCSessionDescription(data.answer));
          console.log('[WEBRTC] ‚úÖ ========== REMOTE ANSWER SET ==========');
          console.log('[WEBRTC] ‚úÖ Updated signaling state:', peerConnection.signalingState);
          console.log('[WEBRTC] ‚úÖ Connection state:', peerConnection.connectionState);
          console.log('[WEBRTC] ‚úÖ ICE connection state:', peerConnection.iceConnectionState);
          
          // Check if we have remote tracks
          const receivers = peerConnection.getReceivers();
          console.log('[WEBRTC] üìä Remote receivers after answer:', {
            totalReceivers: receivers.length,
            receivers: receivers.map(r => ({
              kind: r.track?.kind,
              trackId: r.track?.id,
              trackEnabled: r.track?.enabled,
              trackReadyState: r.track?.readyState,
            })),
          });
        } else {
          console.warn('[WEBRTC] ‚ö†Ô∏è ========== CANNOT SET REMOTE ANSWER ==========');
          console.warn('[WEBRTC] ‚ö†Ô∏è Wrong signaling state:', {
            currentState: peerConnection.signalingState,
            expectedState: 'have-local-offer or stable',
          });
        }
        console.log('[WEBRTC] üì• ========== ANSWER HANDLED ==========');
      } catch (error: any) {
        console.error('[WEBRTC] ‚ùå ========== ERROR HANDLING ANSWER ==========');
        console.error('[WEBRTC] ‚ùå Error details:', {
          message: error.message,
          stack: error.stack,
          name: error.name,
        });
        // Don't show error to user for WebRTC signaling issues
      }
    });

    socket.on('signal:candidate', async (data) => {
      const { peerConnection } = get();
      if (!peerConnection) {
        console.warn('[WEBRTC] ‚ö†Ô∏è Cannot add ICE candidate - no peer connection');
        return;
      }
      
      try {
        if (data.candidate) {
          console.log('[WEBRTC] üì• Received ICE candidate from:', data.fromId || 'unknown');
          console.log('[WEBRTC] üì• Candidate details:', {
            candidate: data.candidate.candidate?.substring(0, 100) + '...',
            sdpMLineIndex: data.candidate.sdpMLineIndex,
            sdpMid: data.candidate.sdpMid,
          });
          console.log('[WEBRTC] üì• Current signaling state:', peerConnection.signalingState);
          
          await peerConnection.addIceCandidate(new RTCIceCandidate(data.candidate));
          console.log('[WEBRTC] ‚úÖ ICE candidate added successfully');
        } else {
          console.log('[WEBRTC] üì• Received null ICE candidate (end of candidates)');
          // This is normal - it signals that all candidates have been sent
        }
      } catch (error: any) {
        // Ignore errors for duplicate or invalid candidates (common in WebRTC)
        if (error.message?.includes('duplicate') || error.message?.includes('Invalid') || error.message?.includes('not in valid')) {
          console.log('[WEBRTC] ‚ÑπÔ∏è Ignoring duplicate/invalid ICE candidate:', error.message);
        } else {
          console.error('[WEBRTC] ‚ùå Error adding ICE candidate:', {
            error: error.message,
            candidate: data.candidate?.candidate?.substring(0, 50),
          });
        }
      }
    });

    socket.on('call:started', (data) => {
      console.log('[CALL] üìû Call started event received:', data);
      const startTime = Date.now();
      set({
        callStatus: 'active',
        callId: data.callId,
        isRecording: true, // Set flag, actual recording will start when streams are ready
        callStartTime: startTime, // Store start time for continuous duration
      });
      // Start speech recognition for transcription
      // Ensure socket is connected first
      const { socket: currentSocket } = get();
      if (currentSocket && currentSocket.connected) {
        setTimeout(() => {
          console.log('[CALL] Starting speech recognition after call:started');
          get().startSpeechRecognition();
          // CRITICAL: Start call recording automatically when call starts
          console.log('[RECORDING] Auto-starting recording after call:started event');
          get().startCallRecording();
        }, 1000); // Wait 1 second for streams to be ready
      } else {
        console.warn('[CALL] ‚ö†Ô∏è Socket not connected, cannot start transcription');
        // Wait for socket and retry
        const checkSocketInterval = setInterval(() => {
          const { socket: checkSocket, localStream } = get();
          if (checkSocket && checkSocket.connected && localStream) {
            console.log('[CALL] Socket connected and streams ready, starting services');
            get().startSpeechRecognition();
            // CRITICAL: Start call recording when socket connects and streams are ready
            console.log('[RECORDING] Auto-starting recording after socket connection');
            get().startCallRecording();
            get().startAIAnalysis();
            clearInterval(checkSocketInterval);
          }
        }, 500);
        setTimeout(() => clearInterval(checkSocketInterval), 10000);
      }
    });

    socket.on('call:ended', async (data) => {
      console.log('[CALL] Call ended for all participants:', data?.reason || 'unknown');
      set({
        callStatus: 'ended',
        isRecording: false,
      });
      // Stop speech recognition
      get().stopSpeechRecognition();
      // Stop AI analysis
      get().stopAIAnalysis();
      // Generate final summary (only when call actually ends for everyone)
      await get().analyzeTranscript(true);
      // Stop and upload recording (only when call actually ends for everyone)
      await get().stopCallRecording();
    });

    socket.on('transcript:chunk', (segment: TranscriptSegment) => {
      console.log('[TRANSCRIPT] ‚úÖ Received transcript chunk:', {
        speaker: segment.speaker,
        speakerId: segment.speakerId,
        text: segment.text.substring(0, 100) + (segment.text.length > 100 ? '...' : ''),
        fullTextLength: segment.text.length,
        timestamp: segment.timestamp,
        currentTime: Date.now(),
        timeDiff: Date.now() - segment.timestamp,
      });
      const authUser = useAuthStore.getState().user;
      const currentUserId = authUser?._id;
      console.log('[TRANSCRIPT] Current user ID:', currentUserId);
      console.log('[TRANSCRIPT] Current transcript length:', get().transcript.length);
      console.log('[TRANSCRIPT] Socket connection state:', socket.connected ? 'connected' : 'disconnected');
      
      // Get current user info to check if this is from current user
      const { userName: currentUserName } = get();
      
      // Normalize speaker name - keep original speaker name from server
      // Server already identifies the speaker correctly, so we should display it as-is
      // Only normalize if it's clearly the current user
      let displaySpeaker = segment.speaker;
      if (segment.speakerId && currentUserId) {
        const segmentUserId = segment.speakerId.toString();
        const currentUserIdStr = currentUserId.toString();
        if (segmentUserId === currentUserIdStr) {
          // This is from current user - use their name or "You"
          displaySpeaker = currentUserName || 'You';
        } else {
          // This is from another user - keep their name as sent by server
          displaySpeaker = segment.speaker;
        }
      }
      
      const normalizedSegment: TranscriptSegment = {
        ...segment,
        speaker: displaySpeaker,
      };
      
      set((state) => {
        // Check if this segment already exists (avoid duplicates)
        // Check by speaker + text + timestamp to be more accurate
        const exists = state.transcript.some(
          s => {
            const sameText = s.text.trim().toLowerCase() === segment.text.trim().toLowerCase();
            const sameSpeaker = s.speaker === displaySpeaker || 
              (segment.speakerId && s.speakerId && s.speakerId.toString() === segment.speakerId.toString());
            const closeTimestamp = Math.abs(s.timestamp - segment.timestamp) < 10000;
            return sameText && sameSpeaker && closeTimestamp;
          }
        );
        if (exists) {
          console.log('[TRANSCRIPT] ‚ö†Ô∏è Duplicate segment, skipping');
          return state;
        }
        
        console.log('[TRANSCRIPT] ‚úÖ Adding new segment from:', displaySpeaker);
        const newTranscript = [...state.transcript, normalizedSegment];
        console.log('[TRANSCRIPT] ‚úÖ Updated transcript length:', newTranscript.length);
        return {
          transcript: newTranscript,
          interimTranscript: '', // Clear interim when final arrives
        };
      });
      
      // Auto-scroll transcript to bottom
      setTimeout(() => {
        const transcriptElement = document.querySelector('[data-transcript-container]');
        if (transcriptElement) {
          transcriptElement.scrollTop = transcriptElement.scrollHeight;
        }
      }, 100);
    });

    // Handle remote participant audio state changes
    socket.on('participant:audio:changed', (data: { socketId: string; userId: string; userName: string; isMuted: boolean }) => {
      console.log('[AUDIO] üì• Remote participant audio state changed:', {
        socketId: data.socketId,
        userId: data.userId,
        userName: data.userName,
        isMuted: data.isMuted,
      });
      
      // Update participant state
      set((state) => ({
        participants: state.participants.map((p) =>
          p.socketId === data.socketId || p.userId === data.userId
            ? { ...p, isMuted: data.isMuted }
            : p
        ),
      }));
      
      // Update remote audio element mute state if needed
      const remoteAudioElement = document.getElementById(`remote-audio-${data.socketId}`) as HTMLAudioElement;
      if (remoteAudioElement) {
        // Note: We can't directly mute remote audio, but we can show visual indicator
        console.log(`[AUDIO] Remote participant ${data.userId} is ${data.isMuted ? 'muted' : 'unmuted'}`);
      }
    });

    // Handle remote participant video state changes
    socket.on('participant:video:changed', (data: { socketId: string; userId: string; userName: string; isVideoOff: boolean }) => {
      console.log('[VIDEO] üì• Remote participant video state changed:', {
        socketId: data.socketId,
        userId: data.userId,
        userName: data.userName,
        isVideoOff: data.isVideoOff,
      });
      
      // Update participant state
      set((state) => ({
        participants: state.participants.map((p) =>
          p.socketId === data.socketId || p.userId === data.userId
            ? { ...p, isVideoOff: data.isVideoOff }
            : p
        ),
      }));
    });

    socket.on('ai:notes', (notes: AINotes) => {
      set({ aiNotes: notes });
    });

    set({ socket });
  },

  disconnectSocket: () => {
    const { socket, peerConnection, localStream } = get();
    
    if (localStream) {
      localStream.getTracks().forEach(track => track.stop());
    }
    
    if (peerConnection) {
      peerConnection.close();
    }
    
    if (socket) {
      socket.disconnect();
    }
    
    set({
      socket: null,
      peerConnection: null,
      localStream: null,
      remoteStream: null,
    });
  },

  createRoom: async (token: string) => {
    set({ error: null });
    
    try {
      const response = await fetch(`${API_URL}/api/rooms`, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${token}`,
          'Content-Type': 'application/json',
        },
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        
        // If token is invalid, clear storage and reload
        if (response.status === 401) {
          localStorage.removeItem('accessToken');
          localStorage.removeItem('refreshToken');
          localStorage.removeItem('user');
          toast.error('Session Expired', 'Please login again.');
          window.location.href = '/login';
          throw new Error('Session expired. Please login again.');
        }
        
        const error = parseApiError(errorData);
        const message = getUserFriendlyMessage(error);
        set({ error: message });
        toast.error('Failed to Create Room', message);
        throw new Error(message);
      }

      const data = await response.json();
      set({ isHost: true, callId: data.callId, error: null });
      toast.success('Room Created', `Room code: ${data.roomId}`);
      return data.roomId;
    } catch (error: any) {
      // Network error - check if server is reachable
      if (error instanceof TypeError && error.message === 'Failed to fetch') {
        console.error('[ROOM] Network error - server may not be running on', API_URL);
        const message = 'Unable to connect to server. Please ensure the backend is running.';
        set({ error: message });
        toast.error('Connection Error', message);
        throw new Error(message);
      }
      console.error('Create room error:', error);
      throw error;
    }
  },

  joinRoom: async (roomId: string, token: string) => {
    set({ error: null, callStatus: 'connecting' });
    
    // Wait for socket to be connected (with timeout)
    const waitForSocket = (): Promise<Socket> => {
      return new Promise((resolve, reject) => {
        const currentSocket = get().socket;
        if (currentSocket && currentSocket.connected) {
          resolve(currentSocket);
          return;
        }

        // Wait up to 10 seconds for socket connection
        const timeout = setTimeout(() => {
          reject(new Error('Socket connection timeout. Please refresh and try again.'));
        }, 10000);

        // Check every 100ms
        const checkInterval = setInterval(() => {
          const checkSocket = get().socket;
          if (checkSocket && checkSocket.connected) {
            clearTimeout(timeout);
            clearInterval(checkInterval);
            resolve(checkSocket);
          }
        }, 100);

        // If socket doesn't exist yet, wait a bit for initSocket to create it
        if (!currentSocket) {
          setTimeout(() => {
            const newSocket = get().socket;
            if (newSocket) {
              newSocket.once('connect', () => {
                clearTimeout(timeout);
                clearInterval(checkInterval);
                resolve(newSocket);
              });
            }
          }, 100);
        } else {
          // Socket exists but not connected, wait for connect event
          currentSocket.once('connect', () => {
            clearTimeout(timeout);
            clearInterval(checkInterval);
            resolve(currentSocket);
          });
        }
      });
    };

    try {
      // Wait for socket connection before proceeding
      console.log('[JOIN] Waiting for socket connection...');
      await waitForSocket();
      console.log('[JOIN] ‚úÖ Socket connected, proceeding with room join');

      const response = await fetch(`${API_URL}/api/rooms/${roomId}/join`, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${token}`,
          'Content-Type': 'application/json',
        },
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        
        if (response.status === 401) {
          localStorage.removeItem('accessToken');
          localStorage.removeItem('refreshToken');
          localStorage.removeItem('user');
          toast.error('Session Expired', 'Please login again.');
          window.location.href = '/login';
          throw new Error('Session expired');
        }
        
        if (response.status === 404) {
          const message = 'Room not found. Please check the room code.';
          set({ error: message });
          toast.error('Room Not Found', message);
          throw new Error(message);
        }
        
        const error = parseApiError(errorData);
        const message = getUserFriendlyMessage(error);
        set({ error: message });
        toast.error('Failed to Join Room', message);
        throw new Error(message);
      }

      const data = await response.json();
      set({
        isHost: data.isHost,
        callId: data.callId,
        roomId,
        error: null,
      });
    } catch (error: any) {
      if (error instanceof TypeError && error.message === 'Failed to fetch') {
        console.error('[ROOM] Network error - server may not be running on', API_URL);
        const message = 'Unable to connect to server. Please ensure the backend is running.';
        set({ error: message, callStatus: 'idle' });
        toast.error('Connection Error', message);
        throw new Error(message);
      }
      if (error.message?.includes('timeout')) {
        set({ error: error.message, callStatus: 'idle' });
        toast.error('Connection Timeout', error.message);
        throw error;
      }
      set({ callStatus: 'idle' });
      throw error;
    }

    // Get media devices with proper error handling
    let stream: MediaStream | null = null;
    try {
      console.log('[WEBRTC] Requesting camera and microphone permissions...');
      stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
        },
        video: {
          width: { ideal: 1280 },
          height: { ideal: 720 },
          facingMode: 'user',
        },
      });
      
      console.log('[WEBRTC] ‚úÖ Media stream obtained:', {
        id: stream.id,
        active: stream.active,
        videoTracks: stream.getVideoTracks().length,
        audioTracks: stream.getAudioTracks().length,
        videoEnabled: stream.getVideoTracks()[0]?.enabled,
        audioEnabled: stream.getAudioTracks()[0]?.enabled,
      });

      // Verify tracks are actually enabled
      stream.getVideoTracks().forEach(track => {
        console.log('[WEBRTC] Video track:', {
          id: track.id,
          enabled: track.enabled,
          readyState: track.readyState,
          muted: track.muted,
        });
      });

      stream.getAudioTracks().forEach(track => {
        console.log('[WEBRTC] Audio track:', {
          id: track.id,
          enabled: track.enabled,
          readyState: track.readyState,
          muted: track.muted,
        });
      });
    } catch (error: any) {
      console.error('[WEBRTC] ‚ùå Error getting user media:', error);
      let errorMessage = 'Failed to access camera/microphone';
      
      if (error.name === 'NotAllowedError' || error.name === 'PermissionDeniedError') {
        errorMessage = 'Camera/microphone permission denied. Please allow access and refresh.';
      } else if (error.name === 'NotFoundError' || error.name === 'DevicesNotFoundError') {
        errorMessage = 'No camera/microphone found. Please connect a device.';
      } else if (error.name === 'NotReadableError' || error.name === 'TrackStartError') {
        errorMessage = 'Camera/microphone is being used by another application.';
      } else if (error.name === 'OverconstrainedError') {
        errorMessage = 'Camera does not support required settings.';
      }
      
      set({ error: errorMessage, callStatus: 'idle' });
      toast.error('Media Access Error', errorMessage);
      throw new Error(errorMessage);
    }

    if (!stream) {
      throw new Error('Failed to get media stream');
    }

    console.log('[WEBRTC] üöÄ Creating RTCPeerConnection with ICE servers:', ICE_SERVERS);
    const pc = new RTCPeerConnection(ICE_SERVERS);

    // CRITICAL: Add all tracks to peer connection BEFORE any signaling
    console.log('[WEBRTC] üì§ Adding local tracks to peer connection...');
    stream.getTracks().forEach(track => {
      console.log('[WEBRTC] ‚ûï Adding track:', {
        kind: track.kind,
        id: track.id,
        enabled: track.enabled,
        readyState: track.readyState,
        muted: track.muted,
        label: track.label,
      });
      
      try {
        if (!stream) {
          console.error('[WEBRTC] ‚ùå Cannot add track - stream is null');
          return;
        }
        const sender = pc.addTrack(track, stream);
        console.log('[WEBRTC] ‚úÖ Track added successfully:', {
          kind: track.kind,
          id: track.id,
          sender: sender ? 'created' : 'null',
        });
      } catch (error: any) {
        console.error('[WEBRTC] ‚ùå Error adding track:', {
          kind: track.kind,
          id: track.id,
          error: error.message,
        });
      }
    });

    // Verify tracks were added
    const senders = pc.getSenders();
    console.log('[WEBRTC] üìä Peer connection senders after adding tracks:', {
      totalSenders: senders.length,
      senders: senders.map(s => ({
        kind: s.track?.kind,
        id: s.track?.id,
        enabled: s.track?.enabled,
      })),
    });

    // Handle incoming remote tracks - CRITICAL for video display
    // CRITICAL: Create a single merged stream that accumulates all tracks from multiple ontrack events
    let mergedRemoteStream: MediaStream | null = null;
    
    pc.ontrack = (event) => {
      console.log('[WEBRTC] üìπ ========== ONTRACK EVENT ==========');
      console.log('[WEBRTC] üìπ Full event details:', {
        streams: event.streams?.length || 0,
        streamIds: event.streams?.map(s => s.id) || [],
        track: event.track ? {
          kind: event.track.kind,
          id: event.track.id,
          enabled: event.track.enabled,
          readyState: event.track.readyState,
          muted: event.track.muted,
          label: event.track.label,
        } : null,
        receiver: event.receiver ? {
          track: event.receiver.track?.kind,
        } : null,
        transceiver: event.transceiver ? {
          mid: event.transceiver.mid,
          direction: event.transceiver.direction,
          currentDirection: event.transceiver.currentDirection,
        } : null,
      });
      
      // CRITICAL: Handle track from event
      if (!event.track) {
        console.error('[WEBRTC] ‚ùå ontrack event has no track - cannot process');
        return;
      }
      
      const track = event.track;
      console.log('[WEBRTC] üéØ Processing track:', {
        kind: track.kind,
        id: track.id,
        enabled: track.enabled,
        readyState: track.readyState,
        muted: track.muted,
      });
      
      // CRITICAL: Get existing stream from store or create new one
      // Multiple ontrack events will fire (one for audio, one for video)
      // We need to merge them into a single stream
      const { remoteStream: existingRemoteStream } = get();
      
      if (!mergedRemoteStream) {
        // Use existing stream from store if available, otherwise create new
        if (existingRemoteStream && existingRemoteStream.active) {
          console.log('[WEBRTC] ‚ôªÔ∏è Reusing existing remote stream from store');
          mergedRemoteStream = existingRemoteStream;
        } else {
          console.log('[WEBRTC] üÜï Creating new merged remote stream');
          mergedRemoteStream = new MediaStream();
        }
      }
      
      // Check if this track is already in the merged stream
      const existingTrack = mergedRemoteStream.getTracks().find(t => t.id === track.id);
      if (existingTrack) {
        console.log('[WEBRTC] ‚ÑπÔ∏è Track already in merged stream, skipping:', {
          trackId: track.id,
          kind: track.kind,
        });
        
        // Even if track exists, update store to trigger re-render
        // This ensures React components see the updated stream
        set({ remoteStream: mergedRemoteStream });
        return;
      } else {
        console.log('[WEBRTC] ‚ûï Adding track to merged stream:', {
          trackId: track.id,
          kind: track.kind,
        });
        mergedRemoteStream.addTrack(track);
        
        // CRITICAL: Create a new stream object with all tracks to ensure React detects the change
        // This is necessary because React uses object reference equality
        const allTracks = mergedRemoteStream.getTracks();
        const newStream = new MediaStream(allTracks);
        mergedRemoteStream = newStream;
        console.log('[WEBRTC] ‚úÖ Created new stream object with all tracks:', {
          streamId: newStream.id,
          totalTracks: allTracks.length,
          trackIds: allTracks.map(t => ({ kind: t.kind, id: t.id })),
        });
      }
      
      // CRITICAL: Verify video track exists and is enabled
      const videoTracks = mergedRemoteStream.getVideoTracks();
      const audioTracks = mergedRemoteStream.getAudioTracks();
      
      console.log('[WEBRTC] üìä Merged remote stream analysis:', {
        streamId: mergedRemoteStream.id,
        totalTracks: mergedRemoteStream.getTracks().length,
        videoTracks: videoTracks.length,
        audioTracks: audioTracks.length,
        streamActive: mergedRemoteStream.active,
      });
      
      if (videoTracks.length > 0) {
        const videoTrack = videoTracks[0];
        console.log('[WEBRTC] üé• Remote video track details:', {
          id: videoTrack.id,
          enabled: videoTrack.enabled,
          readyState: videoTrack.readyState,
          muted: videoTrack.muted,
          label: videoTrack.label,
          settings: videoTrack.getSettings(),
        });
        
        // CRITICAL: Ensure track is enabled
        if (!videoTrack.enabled) {
          console.warn('[WEBRTC] ‚ö†Ô∏è Remote video track is disabled, enabling...');
          videoTrack.enabled = true;
          console.log('[WEBRTC] ‚úÖ Video track enabled');
        }
        
        // Monitor track state changes
        const streamId = mergedRemoteStream.id; // Capture stream ID for closure
        const handleTrackEnded = () => {
          console.log('[WEBRTC] ‚ö†Ô∏è Remote video track ended');
          const { remoteStream: currentStream } = get();
          if (currentStream && currentStream.id === streamId) {
            set({ remoteStream: null });
            mergedRemoteStream = null;
          }
        };
        
        const handleTrackMute = () => {
          console.log('[WEBRTC] ‚ö†Ô∏è Remote video track muted');
        };
        
        const handleTrackUnmute = () => {
          console.log('[WEBRTC] ‚úÖ Remote video track unmuted');
          const { remoteStream: currentStream } = get();
          if (currentStream && currentStream.id === streamId && mergedRemoteStream) {
            set({ remoteStream: mergedRemoteStream });
          }
        };
        
        // Remove old listeners if they exist
        videoTrack.removeEventListener('ended', handleTrackEnded);
        videoTrack.removeEventListener('mute', handleTrackMute);
        videoTrack.removeEventListener('unmute', handleTrackUnmute);
        
        // Add new listeners
        videoTrack.addEventListener('ended', handleTrackEnded);
        videoTrack.addEventListener('mute', handleTrackMute);
        videoTrack.addEventListener('unmute', handleTrackUnmute);
      } else {
        console.log('[WEBRTC] ‚ÑπÔ∏è Remote stream has no video tracks yet (audio-only track received)');
      }
      
      // CRITICAL: Set remote stream in store IMMEDIATELY after each track is added
      // This ensures React components get the updated stream with all tracks
      console.log('[WEBRTC] üíæ Setting merged remote stream in store...');
      set({ remoteStream: mergedRemoteStream });
      console.log('[WEBRTC] ‚úÖ Merged remote stream set in store:', {
        streamId: mergedRemoteStream.id,
        videoTracks: videoTracks.length,
        audioTracks: audioTracks.length,
        shouldTriggerVideoDisplay: videoTracks.length > 0,
      });
      
      // Force update after a short delay to ensure React re-renders
      setTimeout(() => {
        const { remoteStream: currentStream } = get();
        if (currentStream && currentStream.id === mergedRemoteStream?.id) {
          const currentVideoTracks = currentStream.getVideoTracks();
          console.log('[WEBRTC] ‚úÖ Remote stream confirmed in store after delay:', {
            streamId: currentStream.id,
            videoTracks: currentVideoTracks.length,
            audioTracks: currentStream.getAudioTracks().length,
            videoTrackEnabled: currentVideoTracks[0]?.enabled,
            videoTrackReadyState: currentVideoTracks[0]?.readyState,
          });
        } else {
          console.warn('[WEBRTC] ‚ö†Ô∏è Stream mismatch or not set in store:', {
            expectedId: mergedRemoteStream?.id,
            currentId: currentStream?.id,
          });
        }
      }, 100);
      
      console.log('[WEBRTC] üìπ ========== ONTRACK EVENT COMPLETE ==========');
    };

      pc.onicecandidate = (event) => {
        if (event.candidate) {
          console.log('[WEBRTC] üì§ ICE candidate generated:', {
            candidate: event.candidate.candidate?.substring(0, 100) + '...',
            sdpMLineIndex: event.candidate.sdpMLineIndex,
            sdpMid: event.candidate.sdpMid,
          });
          
          const { socket: currentSocket, participants } = get();
          if (currentSocket && currentSocket.connected) {
            // Send to all existing participants
            if (participants.length > 0) {
              participants.forEach(p => {
                console.log('[WEBRTC] üì§ Sending ICE candidate to participant:', p.socketId);
                currentSocket.emit('signal:candidate', {
                  targetId: p.socketId,
                  candidate: event.candidate,
                });
              });
            } else {
              // No participants yet - this is normal, candidates will be sent when participants join
              // ICE candidates are generated continuously, so we'll catch up when participants arrive
              console.log('[WEBRTC] üì¶ ICE candidate generated but no participants yet (will send when participants join)');
            }
          } else {
            console.warn('[WEBRTC] ‚ö†Ô∏è Socket not connected, cannot send ICE candidate');
          }
        } else {
          // null candidate means end of candidates
          console.log('[WEBRTC] ‚úÖ All ICE candidates gathered (null candidate received)');
          
          // Send null candidate to all participants to signal end of candidates
          const { socket: currentSocket, participants } = get();
          if (currentSocket && currentSocket.connected && participants.length > 0) {
            participants.forEach(p => {
              console.log('[WEBRTC] üì§ Sending null ICE candidate (end of candidates) to:', p.socketId);
              currentSocket.emit('signal:candidate', {
                targetId: p.socketId,
                candidate: null,
              });
            });
          }
        }
      };

      pc.onconnectionstatechange = () => {
        console.log('[WEBRTC] üîÑ Connection state changed:', {
          connectionState: pc.connectionState,
          iceConnectionState: pc.iceConnectionState,
          iceGatheringState: pc.iceGatheringState,
          signalingState: pc.signalingState,
        });
        
        if (pc.connectionState === 'connected') {
          console.log('[WEBRTC] ‚úÖ Peer connection CONNECTED - video should be working now');
          const { callStartTime } = get();
          // Only set start time if not already set (preserve existing start time)
          const startTime = callStartTime || Date.now();
          set({ 
            callStatus: 'active', 
            isRecording: true,
            callStartTime: startTime, // Store start time for continuous duration
          });
      // Start speech recognition for transcription when connected
          // But only if socket is also connected and user is not muted
      setTimeout(() => {
            const { socket: currentSocket, isMuted } = get();
            if (currentSocket && currentSocket.connected && !isMuted) {
          console.log('[SPEECH] Starting speech recognition after WebRTC connection');
          get().startSpeechRecognition();
          // Start call recording
          get().startCallRecording();
        } else {
              console.warn('[SPEECH] ‚ö†Ô∏è Socket not connected or user is muted, waiting...');
              // Wait for socket connection and unmute
              const checkSocketInterval = setInterval(() => {
                const { socket: checkSocket, isMuted: checkMuted } = get();
                if (checkSocket && checkSocket.connected && !checkMuted) {
                  console.log('[SPEECH] Socket connected and unmuted, starting recognition now');
              get().startSpeechRecognition();
              get().startCallRecording();
                  clearInterval(checkSocketInterval);
            }
          }, 500);
          
          // Stop checking after 10 seconds
              setTimeout(() => clearInterval(checkSocketInterval), 10000);
        }
      }, 1000); // Wait 1 second for stream to stabilize
        } else if (pc.connectionState === 'failed') {
          toast.error('Connection Failed', 'Unable to establish call connection');
          set({ error: 'Connection failed', callStatus: 'idle' });
          get().stopSpeechRecognition();
        } else if (pc.connectionState === 'disconnected') {
          toast.warning('Connection Lost', 'Trying to reconnect...');
          get().stopSpeechRecognition();
        }
      };

      pc.onicecandidateerror = (event) => {
        console.error('ICE candidate error:', event);
      };

      set({
        peerConnection: pc, // Keep for backward compatibility
        peerConnections: new Map(), // Initialize empty Map for mesh topology
        localStream: stream,
        remoteStreams: new Map(), // Initialize empty Map for mesh topology
        pendingIceCandidates: new Map(), // Initialize empty Map for queued ICE candidates
        // CRITICAL: Initialize mute/video state from actual track state
        // If no track exists, default to muted/off (true). Otherwise, check if track is enabled.
        isMuted: stream.getAudioTracks().length === 0 || !stream.getAudioTracks()[0]?.enabled,
        isVideoOff: stream.getVideoTracks().length === 0 || !stream.getVideoTracks()[0]?.enabled,
      });

      // Ensure socket is connected before emitting room:join (with userId/userName for mesh topology)
      const { socket: currentSocket, userName } = get();
      const authUser = useAuthStore.getState().user;
      const userId = authUser?._id || currentSocket?.id;
      
      if (currentSocket && currentSocket.connected) {
        console.log('[JOIN] ‚úÖ Emitting room:join with connected socket (mesh topology)');
        currentSocket.emit('room:join', { 
          roomId,
          userId: userId || `user-${currentSocket.id}`,
          userName: userName || 'User'
        });
      } else {
        console.warn('[JOIN] ‚ö†Ô∏è Socket not connected, waiting...');
        // Wait for socket and then emit
        const waitAndEmit = setInterval(() => {
          const { socket: checkSocket, userName: checkUserName } = get();
          if (checkSocket && checkSocket.connected) {
            const authUser = useAuthStore.getState().user;
            const userId = authUser?._id || checkSocket.id;
            console.log('[JOIN] ‚úÖ Socket connected, emitting room:join now (mesh topology)');
            checkSocket.emit('room:join', { 
              roomId,
              userId: userId || `user-${checkSocket.id}`,
              userName: checkUserName || 'User'
            });
            clearInterval(waitAndEmit);
          }
        }, 100);
        setTimeout(() => clearInterval(waitAndEmit), 10000); // Timeout after 10s
      }

      // Start speech recognition immediately after getting stream (only if not muted)
      // This allows transcription even before other participants join
      // IMPORTANT: Both users need speech recognition running to see each other's transcripts
      // Each user's browser captures their own microphone, so both must have recognition active
      setTimeout(() => {
        const { isMuted, speechRecognition, callStatus } = get();
        if (!isMuted && !speechRecognition && callStatus !== 'ended') {
          console.log('[JOIN] ‚úÖ Starting speech recognition after getting media stream');
          console.log('[JOIN] ‚ö†Ô∏è IMPORTANT: Both users must have speech recognition active to see each other\'s transcripts');
        get().startSpeechRecognition();
        } else if (isMuted) {
          console.log('[JOIN] ‚ö†Ô∏è User is muted, skipping speech recognition (will start when unmuted)');
          console.log('[JOIN] ‚ö†Ô∏è REMINDER: Unmute to enable transcription for this user');
        } else if (speechRecognition) {
          console.log('[JOIN] ‚úÖ Speech recognition already active');
        } else if (callStatus === 'ended') {
          console.log('[JOIN] ‚ö†Ô∏è Call ended, not starting speech recognition');
        }
      }, 1000);
  },

  leaveRoom: async () => {
    const { socket, roomId, localStream, peerConnection, peerConnections } = get();
    
    console.log('[LEAVE] Leaving room:', roomId);
    
    // Stop speech recognition first
    get().stopSpeechRecognition();
    // Stop recording
    await get().stopCallRecording();
    
    // Emit leave event (but don't disconnect socket - let it reconnect)
    if (socket && roomId && socket.connected) {
      console.log('[LEAVE] Emitting leave-room event (mesh topology)');
      socket.emit('leave-room', { roomId });
    }
    
    // Stop all media tracks immediately
    if (localStream) {
      localStream.getTracks().forEach(track => {
        track.stop();
        console.log('[LEAVE] Stopped track:', track.kind);
      });
    }
    
      // Close all peer connections (mesh topology cleanup)
      peerConnections.forEach((pc, socketId) => {
        pc.close();
        console.log(`[LEAVE] Closed peer connection to ${socketId}`);
      });

      // Clean up monitoring intervals
      const { monitoringIntervals } = get();
      monitoringIntervals.forEach((intervalId, socketId) => {
        clearInterval(intervalId);
        console.log(`[LEAVE] Stopped monitoring for ${socketId}`);
      });
    
    // Close legacy peer connection
    if (peerConnection) {
      peerConnection.close();
      console.log('[LEAVE] Closed legacy peer connection');
    }
    
    // Don't disconnect socket - let it stay connected for reconnection
    // Only disconnect if explicitly requested (e.g., logout)
    
    // Reset call state but keep socket for potential reconnection
    set({
      peerConnection: null,
      peerConnections: new Map(),
      localStream: null,
      remoteStream: null,
      remoteStreams: new Map(),
      pendingIceCandidates: new Map(),
      roomId: null,
      callId: null,
      isHost: false,
      callStatus: 'idle',
      participants: [],
      transcript: [],
      interimTranscript: '',
      aiNotes: null,
      isMuted: false,
      isVideoOff: false,
      isRecording: false,
      error: null,
    });
    
    console.log('[LEAVE] Room left, socket remains connected for reconnection');
  },

  endCall: async () => {
    const { socket, localStream, peerConnection, roomId } = get();
    
    console.log('[CALL] User leaving call, but call continues for others');
    
    // Stop speech recognition for this user only
    get().stopSpeechRecognition();
    // Stop AI analysis for this user only
    get().stopAIAnalysis();
    
    // Stop all media tracks for this user
    if (localStream) {
      localStream.getTracks().forEach(track => {
        track.stop();
        console.log('[CALL] Stopped track:', track.kind);
      });
    }
    
    // Close peer connection for this user
    if (peerConnection) {
      peerConnection.close();
    }
    
    // Emit call:end to remove this user from the call
    // The call will continue for other participants
    if (socket && roomId) {
      socket.emit('call:end');
    }
    
    // Leave the room (this will trigger user:left event for others)
    get().leaveRoom();
    
    // Clear local state (this user has left, but call may continue for others)
    set({ 
      callStatus: 'ended', // This user's call has ended
      isRecording: false,
      isMinimized: false,
      callStartTime: null,
      localStream: null,
      remoteStream: null,
      peerConnection: null,
      participants: [], // Clear participants list for this user
    });
  },

  toggleMute: () => {
    const { localStream, speechRecognition, socket, roomId, peerConnection, peerConnections } = get();
    
    if (!localStream) {
      console.warn('[MUTE] ‚ö†Ô∏è No local stream available');
      return;
    }
    
    // Get actual track state (more reliable than store state)
    const audioTracks = localStream.getAudioTracks();
    if (audioTracks.length === 0) {
      console.warn('[MUTE] ‚ö†Ô∏è No audio tracks available');
      return;
    }
    
    const audioTrack = audioTracks[0];
    
    // Check current track state BEFORE toggling
    const currentTrackEnabled = audioTrack.enabled;
    const newTrackEnabled = !currentTrackEnabled;
    const newMutedState = !newTrackEnabled; // Muted = track disabled
    
    console.log('[MUTE] üîÑ Before toggle - Track enabled:', currentTrackEnabled, 'State muted:', !currentTrackEnabled);
    
    // CRITICAL: Update track state (enabled = unmuted, disabled = muted)
    audioTrack.enabled = newTrackEnabled;
    
    console.log('[MUTE] ‚úÖ After toggle - Track enabled:', audioTrack.enabled, 'State muted:', newMutedState);
    
    // Update all audio tracks in the stream
    audioTracks.forEach(track => {
      if (track.id !== audioTrack.id) {
        track.enabled = newTrackEnabled;
      }
    });
    
    // Update peer connection senders to reflect track state
    if (peerConnection) {
      const senders = peerConnection.getSenders();
      const audioSender = senders.find(s => s.track?.kind === 'audio');
      if (audioSender && audioSender.track) {
        // Ensure sender track state matches
        audioSender.track.enabled = newTrackEnabled;
        console.log('[MUTE] üì° Peer connection audio sender track enabled:', audioSender.track.enabled);
      }
    }
    
    // Update all peer connections (for multi-participant calls)
    peerConnections.forEach((pc, socketId) => {
      const senders = pc.getSenders();
      const audioSender = senders.find(s => s.track?.kind === 'audio');
      if (audioSender && audioSender.track) {
        audioSender.track.enabled = newTrackEnabled;
        console.log(`[MUTE] üì° Peer connection ${socketId} audio sender track enabled:`, audioSender.track.enabled);
      }
    });
    
    // Update store state to match track state IMMEDIATELY
    set({ isMuted: newMutedState });
    
    // Notify other participants of audio state change
    if (socket && roomId) {
      socket.emit('participant:audio:toggle', { isMuted: newMutedState });
      console.log('[MUTE] üì§ Emitted state change to room:', { roomId, isMuted: newMutedState });
    }
    
    // Stop speech recognition when muted, restart when unmuted
    if (newMutedState) {
      // Muted - stop recognition
      if (speechRecognition) {
        console.log('[MUTE] ‚è∏Ô∏è Stopping speech recognition (muted)');
        get().stopSpeechRecognition();
      }
    } else {
      // Unmuted - start recognition if call is active
      const { callStatus } = get();
      if (callStatus === 'active' && !speechRecognition) {
        console.log('[MUTE] ‚ñ∂Ô∏è Starting speech recognition (unmuted)');
        setTimeout(() => {
          get().startSpeechRecognition();
        }, 500);
      }
    }
  },

  toggleVideo: () => {
    const { localStream, socket, roomId, peerConnection, peerConnections } = get();
    
    if (!localStream) {
      console.warn('[VIDEO] ‚ö†Ô∏è No local stream available');
      return;
    }
    
    // Get actual track state (more reliable than store state)
    const videoTracks = localStream.getVideoTracks();
    if (videoTracks.length === 0) {
      console.warn('[VIDEO] ‚ö†Ô∏è No video tracks available');
      return;
    }
    
    const videoTrack = videoTracks[0];
    
    // Check current track state BEFORE toggling
    const currentTrackEnabled = videoTrack.enabled;
    const newTrackEnabled = !currentTrackEnabled;
    const newVideoOff = !newTrackEnabled; // Video off = track disabled
    
    console.log('[VIDEO] üîÑ Before toggle - Track enabled:', currentTrackEnabled, 'State videoOff:', !currentTrackEnabled);
    
    // CRITICAL: Update track state (enabled = video on, disabled = video off)
    videoTrack.enabled = newTrackEnabled;
    
    console.log('[VIDEO] ‚úÖ After toggle - Track enabled:', videoTrack.enabled, 'State videoOff:', newVideoOff);
    
    // Update all video tracks in the stream
    videoTracks.forEach(track => {
      if (track.id !== videoTrack.id) {
        track.enabled = newTrackEnabled;
      }
    });
    
    // Update peer connection senders to reflect track state
    if (peerConnection) {
      const senders = peerConnection.getSenders();
      const videoSender = senders.find(s => s.track?.kind === 'video');
      if (videoSender && videoSender.track) {
        // Ensure sender track state matches
        videoSender.track.enabled = newTrackEnabled;
        console.log('[VIDEO] üì° Peer connection video sender track enabled:', videoSender.track.enabled);
      }
    }
    
    // Update all peer connections (for multi-participant calls)
    peerConnections.forEach((pc, socketId) => {
      const senders = pc.getSenders();
      const videoSender = senders.find(s => s.track?.kind === 'video');
      if (videoSender && videoSender.track) {
        videoSender.track.enabled = newTrackEnabled;
        console.log(`[VIDEO] üì° Peer connection ${socketId} video sender track enabled:`, videoSender.track.enabled);
      }
    });
    
    // Update store state to match track state IMMEDIATELY
    set({ isVideoOff: newVideoOff });
    
    // Notify other participants of video state change
    if (socket && roomId) {
      socket.emit('participant:video:toggle', { isVideoOff: newVideoOff });
      console.log('[VIDEO] üì§ Emitted state change to room:', { roomId, isVideoOff: newVideoOff });
    }
  },

  sendTranscript: (text: string) => {
    const { socket } = get();
    
    if (socket && text.trim()) {
      socket.emit('transcript:manual', {
        text: text.trim(),
        timestamp: Date.now(),
      });
    }
  },

  requestNotes: () => {
    const { socket } = get();
    
    if (socket) {
      socket.emit('notes:request');
    }
  },

  startAIAnalysis: () => {
    const { aiAnalysisInterval, callStatus } = get();
    
    // Don't start if already running or call not active
    if (aiAnalysisInterval || callStatus !== 'active') {
      return;
    }

    console.log('[AI] Starting real-time AI analysis (every 30 seconds)');
    
    // Start immediate analysis
    get().analyzeTranscript(false);
    
    // Set up interval for periodic analysis
    const interval = setInterval(() => {
      const { callStatus: currentStatus } = get();
      if (currentStatus === 'active') {
        get().analyzeTranscript(false);
      } else {
        // Stop if call ended
        get().stopAIAnalysis();
      }
    }, 30000); // Every 30 seconds

    set({ aiAnalysisInterval: interval });
  },

  stopAIAnalysis: () => {
    const { aiAnalysisInterval } = get();
    
    if (aiAnalysisInterval) {
      clearInterval(aiAnalysisInterval);
      set({ aiAnalysisInterval: null });
      console.log('[AI] Stopped real-time AI analysis');
    }
  },

  analyzeTranscript: async (isFinal: boolean = false) => {
    const { transcript, callId, participants, callStartTime } = get();
    const { accessToken } = useAuthStore.getState();
    
    if (!callId || !accessToken) {
      console.warn('[AI] Cannot analyze: missing callId or accessToken');
      return;
    }

    if (transcript.length === 0) {
      console.log('[AI] No transcript to analyze yet');
      return;
    }

    try {
      // Convert transcript segments to text
      const transcriptText = transcript
        .map(seg => `${seg.speaker}: ${seg.text}`)
        .join('\n');

      // Calculate duration
      const duration = callStartTime ? Date.now() - callStartTime : 0;

      // Get participant names
      const participantNames = participants.map(p => p.userName);
      const authUser = useAuthStore.getState().user;
      if (authUser?.name && !participantNames.includes(authUser.name)) {
        participantNames.push(authUser.name);
      }

      console.log('[AI] Analyzing transcript...', {
        transcriptLength: transcriptText.length,
        segmentCount: transcript.length,
        isFinal,
      });

      const response = await fetch(`${API_URL}/api/calls/${callId}/analyze-transcript`, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${accessToken}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          transcript: transcriptText,
          participants: participantNames,
          duration,
          isFinal,
        }),
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        console.error('[AI] Analysis failed:', errorData);
        return;
      }

      const analysis = await response.json();
      
      // Update AI notes in store
      set({
        aiNotes: {
          summary: analysis.summary || '',
          bullets: analysis.keyPoints || [],
          actionItems: analysis.actionItems || [],
          decisions: analysis.decisions || [],
          suggestedReplies: analysis.nextSteps || [],
          keyTopics: analysis.topics || [],
          isFinal,
          lastUpdated: Date.now(),
        },
      });

      console.log('[AI] ‚úÖ Analysis complete', { isFinal });
    } catch (error: any) {
      console.error('[AI] Error analyzing transcript:', error);
    }
  },

  startSpeechRecognition: () => {
    const { socket, speechRecognition } = get();
    
    // Don't start if already active
    if (speechRecognition) {
      console.log('[SPEECH] Speech recognition already active');
      return;
    }
    
    // Allow speech recognition even without socket (for local display)
    if (!socket || !socket.connected) {
      console.warn('[SPEECH] ‚ö†Ô∏è No socket connection - will work locally only');
      // Continue anyway - we'll show transcript locally
    }
    
    // Check Web Speech API support
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    if (!SpeechRecognition) {
      console.error('[SPEECH] ‚ùå Web Speech API not supported in this browser');
      console.error('[SPEECH] Available:', {
        SpeechRecognition: typeof window.SpeechRecognition,
        webkitSpeechRecognition: typeof window.webkitSpeechRecognition,
        userAgent: navigator.userAgent,
      });
      toast.error('Speech Recognition Not Supported', 'Please use Chrome or Edge browser. Web Speech API is not available in this browser.');
      return;
    }
    
    console.log('[SPEECH] ‚úÖ Web Speech API is available');
    
    try {
      const recognition = new SpeechRecognition();
      
      // Configuration
      recognition.continuous = true; // Keep listening
      recognition.interimResults = true; // Show interim results
      recognition.lang = 'en-US'; // Language
      recognition.maxAlternatives = 1; // Only get best result
      
      recognition.onstart = () => {
        console.log('[SPEECH] ‚úÖ Speech recognition started');
        console.log('[SPEECH] Language:', recognition.lang);
        console.log('[SPEECH] Continuous:', recognition.continuous);
        console.log('[SPEECH] Interim results:', recognition.interimResults);
        console.log('[SPEECH] Max alternatives:', recognition.maxAlternatives);
        set({ speechRecognition: recognition }); // Store immediately
        toast.success('Speech Recognition Active', 'Listening for speech...');
      };
      
      recognition.onresult = (event: any) => {
        let finalTranscript = '';
        let interimText = '';
        
        console.log('[SPEECH] üìù onresult event:', {
          resultIndex: event.resultIndex,
          resultsLength: event.results.length,
        });
        
        // Process all results
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const result = event.results[i];
          const transcript = result[0]?.transcript || '';
          const isFinal = result.isFinal;
          
          console.log('[SPEECH] Result', i, ':', {
            transcript,
            isFinal,
            confidence: result[0]?.confidence,
          });
          
          if (isFinal) {
            finalTranscript += transcript + ' ';
          } else {
            interimText += transcript + ' ';
          }
        }
        
        // Update interim transcript in UI immediately for live feedback
        if (interimText.trim()) {
          console.log('[SPEECH] üìù Interim transcript:', interimText.trim());
          set({ interimTranscript: interimText.trim() });
        } else if (finalTranscript.trim()) {
          // Clear interim when we have final
          set({ interimTranscript: '' });
        }
        
        // Send final transcript to server and show locally as fallback
        if (finalTranscript.trim()) {
          console.log('[SPEECH] ‚úÖ Final transcript:', finalTranscript.trim());
          const { socket: currentSocket, roomId, isMuted, userName: currentUserName } = get();
          const authUser = useAuthStore.getState().user;
          
          // Don't send if muted
          if (isMuted) {
            console.log('[SPEECH] ‚ö†Ô∏è User is muted, ignoring transcript');
            return;
          }
          
          // Create segment for local display (fallback if server doesn't respond)
          const localSegment: TranscriptSegment = {
            speaker: currentUserName || 'You',
            speakerId: authUser?._id,
            text: finalTranscript.trim(),
            timestamp: Date.now(),
          };
          
          // Show locally immediately for better UX
          set((state) => {
            // Check if already exists
            const exists = state.transcript.some(
              s => s.text.trim().toLowerCase() === localSegment.text.trim().toLowerCase() 
                && Math.abs(s.timestamp - localSegment.timestamp) < 5000
            );
            if (!exists) {
              console.log('[SPEECH] ‚úÖ Adding transcript locally (will be replaced by server response)');
              return {
                transcript: [...state.transcript, localSegment],
                interimTranscript: '', // Clear interim
              };
            }
            return state;
          });
          
          // Send to server - server will broadcast back and replace/update the local one
          if (currentSocket && currentSocket.connected && roomId) {
            console.log('[SPEECH] üì§ Sending transcript to server:', {
              text: finalTranscript.trim(),
              roomId,
              socketConnected: currentSocket.connected,
            });
            
            try {
              currentSocket.emit('transcript:manual', {
                text: finalTranscript.trim(),
                timestamp: Date.now(),
              }, (response: any) => {
                if (response) {
                  console.log('[SPEECH] ‚úÖ Server acknowledged:', response);
                }
              });
            } catch (error: any) {
              console.error('[SPEECH] ‚ùå Error sending to server:', error.message);
            }
          } else {
            console.warn('[SPEECH] ‚ö†Ô∏è Socket not available - showing transcript locally only');
            if (!currentSocket) {
              console.warn('[SPEECH]   - No socket instance');
            } else if (!currentSocket.connected) {
              console.warn('[SPEECH]   - Socket not connected');
            } else if (!roomId) {
              console.warn('[SPEECH]   - No roomId');
            }
          }
          
          finalTranscript = '';
        }
      };
      
      recognition.onerror = (event: any) => {
        console.error('[SPEECH] Recognition error:', event.error);
        console.error('[SPEECH] Error details:', event);
        
        if (event.error === 'no-speech') {
          // This is normal, just continue - don't log as error
          console.log('[SPEECH] No speech detected (normal - will continue listening)');
          return;
        }
        
        if (event.error === 'audio-capture') {
          console.error('[SPEECH] ‚ùå Microphone not accessible');
          toast.error('Microphone Error', 'Could not access microphone. Please check permissions.');
          // Try to restart after a delay
          setTimeout(() => {
            const { callStatus } = get();
            if (callStatus === 'active') {
              console.log('[SPEECH] Attempting to restart after audio-capture error');
              get().startSpeechRecognition();
            }
          }, 2000);
        } else if (event.error === 'not-allowed') {
          console.error('[SPEECH] ‚ùå Microphone permission denied');
          toast.error('Permission Denied', 'Microphone permission required. Please allow microphone access.');
          set({ speechRecognition: null });
        } else if (event.error === 'network') {
          console.error('[SPEECH] ‚ùå Network error');
          // Don't show error toast for network issues - might be temporary
          console.log('[SPEECH] Network error - will retry on next recognition cycle');
        } else if (event.error === 'service-not-allowed') {
          console.error('[SPEECH] ‚ùå Speech recognition service not allowed');
          toast.error('Service Not Allowed', 'Please enable speech recognition in browser settings');
          set({ speechRecognition: null });
        } else if (event.error === 'aborted') {
          // Aborted is usually intentional, don't treat as error
          console.log('[SPEECH] Recognition aborted (normal)');
          return;
        } else {
          console.error('[SPEECH] ‚ùå Unknown error:', event.error);
          // Don't show toast for unknown errors - might be temporary
          console.log('[SPEECH] Will continue listening despite error');
        }
        
        // Only stop on critical errors that can't be recovered
        if (event.error === 'not-allowed' || event.error === 'service-not-allowed') {
          set({ speechRecognition: null });
        }
      };
      
      recognition.onend = () => {
        console.log('[SPEECH] Speech recognition ended');
        const { callStatus, speechRecognition: currentRec, localStream, isMuted } = get();
        
        // Don't restart if muted
        if (isMuted) {
          console.log('[SPEECH] ‚è∏Ô∏è Not restarting - user is muted');
          if (currentRec === recognition) {
            set({ speechRecognition: null });
          }
          return;
        }
        
        // Auto-restart if call is still active (Web Speech API auto-stops after ~60s of silence)
        if (callStatus === 'active' && currentRec === recognition && localStream) {
          console.log('[SPEECH] üîÑ Auto-restarting recognition (normal behavior - Web Speech API auto-stops)...');
          setTimeout(() => {
            const { speechRecognition: stillActive, callStatus: stillActiveStatus, localStream: stillStream, isMuted: stillMuted } = get();
            // Only restart if no other recognition instance is active, call is still active, and not muted
            if (!stillActive && stillActiveStatus === 'active' && stillStream && !stillMuted) {
              console.log('[SPEECH] ‚úÖ Restarting speech recognition');
              try {
              get().startSpeechRecognition();
              } catch (error) {
                console.error('[SPEECH] Error restarting recognition:', error);
                // Try again after a longer delay
                setTimeout(() => {
                  const { callStatus: retryStatus, localStream: retryStream, isMuted: retryMuted } = get();
                  if (retryStatus === 'active' && retryStream && !retryMuted) {
                    console.log('[SPEECH] Retrying recognition restart...');
                    get().startSpeechRecognition();
                  }
                }, 2000);
              }
            } else {
              console.log('[SPEECH] ‚è∏Ô∏è Not restarting - recognition already active, call ended, no stream, or muted');
            }
          }, 300); // Wait 300ms before restarting (faster restart)
        } else {
          console.log('[SPEECH] ‚èπÔ∏è Not restarting - call status:', callStatus, 'has stream:', !!localStream);
          if (currentRec === recognition) {
          set({ speechRecognition: null });
          }
        }
      };
      
      // Start recognition
      try {
      recognition.start();
      set({ speechRecognition: recognition });
      console.log('[SPEECH] ‚úÖ Started Web Speech API recognition');
      } catch (error: any) {
        // If already started, that's okay
        if (error.message?.includes('already started') || error.name === 'InvalidStateError') {
          console.log('[SPEECH] Recognition already started, continuing...');
          set({ speechRecognition: recognition });
        } else {
          throw error;
        }
      }
      
    } catch (error: any) {
      console.error('[SPEECH] Failed to start speech recognition:', error.message);
      toast.error('Failed to Start Recognition', error.message);
      set({ speechRecognition: null });
    }
  },

  stopSpeechRecognition: () => {
    const { speechRecognition } = get();
    
    if (speechRecognition) {
      try {
        speechRecognition.stop();
        set({ speechRecognition: null });
        console.log('[SPEECH] ‚úÖ Speech recognition stopped');
      } catch (error) {
        console.error('[SPEECH] Error stopping recognition:', error);
        set({ speechRecognition: null });
      }
    }
  },

  startCallRecording: () => {
    const { localStream, remoteStream, callId, callRecorder } = get();
    
    // Don't start if already recording
    if (callRecorder && callRecorder.state !== 'inactive') {
      console.log('[RECORDING] Already recording, state:', callRecorder.state);
      return;
    }
    
    if (!callId) {
      console.log('[RECORDING] ‚ö†Ô∏è No call ID, skipping recording');
      return;
    }
    
    if (!localStream) {
      console.log('[RECORDING] ‚ö†Ô∏è No local stream available');
      return;
    }
    
    // Check MediaRecorder support
    if (!window.MediaRecorder) {
      console.warn('[RECORDING] ‚ùå MediaRecorder not supported in this browser');
      toast.error('Recording Not Supported', 'MediaRecorder API is not available');
      return;
    }
    
    try {
      console.log('[RECORDING] üé¨ Starting call recording...');
      console.log('[RECORDING] Streams available:', {
        hasLocalStream: !!localStream,
        localVideoTracks: localStream.getVideoTracks().length,
        localAudioTracks: localStream.getAudioTracks().length,
        hasRemoteStream: !!remoteStream,
        remoteVideoTracks: remoteStream?.getVideoTracks().length || 0,
        remoteAudioTracks: remoteStream?.getAudioTracks().length || 0,
      });
      
      // CRITICAL: Combine local and remote streams for recording
      const mixedStream = new MediaStream();
      
      // Add local stream tracks (user's own video/audio)
      localStream.getTracks().forEach(track => {
        if (track.readyState === 'live' && (track.kind === 'audio' || track.kind === 'video')) {
          console.log('[RECORDING] ‚ûï Adding local track:', track.kind, track.id);
          mixedStream.addTrack(track);
        }
      });
      
      // Add remote stream tracks (other participants' video/audio)
      if (remoteStream) {
        remoteStream.getTracks().forEach(track => {
          if (track.readyState === 'live' && (track.kind === 'audio' || track.kind === 'video')) {
            console.log('[RECORDING] ‚ûï Adding remote track:', track.kind, track.id);
            mixedStream.addTrack(track);
          }
        });
      }
      
      const totalTracks = mixedStream.getTracks().length;
      const videoTracks = mixedStream.getVideoTracks().length;
      const audioTracks = mixedStream.getAudioTracks().length;
      
      console.log('[RECORDING] üìä Mixed stream tracks:', {
        totalTracks,
        videoTracks,
        audioTracks,
      });
      
      if (totalTracks === 0) {
        console.warn('[RECORDING] ‚ö†Ô∏è No active tracks available for recording');
        toast.warning('Recording Warning', 'No active media tracks to record');
        return;
      }
      
      // Find supported mime type (prefer VP9 for better compression)
      const supportedTypes = [
        'video/webm;codecs=vp9,opus', // Best compression
        'video/webm;codecs=vp8,opus', // Good compression
        'video/webm', // Fallback
        'audio/webm;codecs=opus', // Audio-only fallback
      ];
      
      let selectedMimeType = '';
      for (const mimeType of supportedTypes) {
        if (MediaRecorder.isTypeSupported(mimeType)) {
          selectedMimeType = mimeType;
          console.log('[RECORDING] ‚úÖ Selected codec:', mimeType);
          break;
        }
      }
      
      if (!selectedMimeType) {
        console.warn('[RECORDING] ‚ö†Ô∏è No supported codec found, using default');
        selectedMimeType = 'video/webm';
      }
      
      // Create MediaRecorder with selected codec
      const recorder = new MediaRecorder(mixedStream, {
        mimeType: selectedMimeType,
        videoBitsPerSecond: 2500000, // 2.5 Mbps for good quality/size balance
      });
      
      // CRITICAL: Clear previous chunks and store in state
      set({ recordingChunks: [] });
      
      recorder.ondataavailable = (event) => {
        if (event.data && event.data.size > 0) {
          console.log('[RECORDING] üì¶ Chunk received:', {
            size: event.data.size,
            type: event.data.type,
          });
          set((state) => ({
            recordingChunks: [...state.recordingChunks, event.data],
          }));
        }
      };
      
      recorder.onerror = (event: any) => {
        console.error('[RECORDING] ‚ùå Recording error:', event.error);
        toast.error('Recording Error', 'An error occurred while recording');
        set({ callRecorder: null, isRecording: false });
      };
      
      recorder.onstop = async () => {
        console.log('[RECORDING] ‚èπÔ∏è Recording stopped, preparing upload...');
        const { callId: currentCallId, recordingChunks: finalChunks } = get();
        
        if (finalChunks.length === 0) {
          console.warn('[RECORDING] ‚ö†Ô∏è No recording chunks collected');
          set({ callRecorder: null, isRecording: false, recordingChunks: [] });
          return;
        }
        
        if (!currentCallId) {
          console.error('[RECORDING] ‚ùå No call ID for upload');
          set({ callRecorder: null, isRecording: false, recordingChunks: [] });
          return;
        }
        
        try {
          console.log('[RECORDING] üì§ Creating blob from', finalChunks.length, 'chunks...');
          const blob = new Blob(finalChunks, { type: selectedMimeType || 'video/webm' });
          console.log('[RECORDING] ‚úÖ Blob created:', {
            size: blob.size,
            type: blob.type,
            sizeMB: (blob.size / (1024 * 1024)).toFixed(2),
          });
          
          const formData = new FormData();
          const filename = `recording-${currentCallId}-${Date.now()}.webm`;
          formData.append('recording', blob, filename);
          
          const token = localStorage.getItem('accessToken');
          if (!token) {
            console.error('[RECORDING] ‚ùå No access token available');
            toast.error('Upload Failed', 'Authentication required');
            set({ callRecorder: null, isRecording: false, recordingChunks: [] });
            return;
          }
          
          console.log('[RECORDING] üì§ Uploading to server...');
          const response = await fetch(`${API_URL}/api/calls/${currentCallId}/recording`, {
            method: 'POST',
            headers: {
              'Authorization': `Bearer ${token}`,
            },
            body: formData,
          });
          
          if (response.ok) {
            const result = await response.json();
            console.log('[RECORDING] ‚úÖ Recording uploaded successfully:', result);
            toast.success('Recording Saved', 'Call recording has been saved successfully');
          } else {
            const errorText = await response.text();
            console.error('[RECORDING] ‚ùå Upload failed:', {
              status: response.status,
              statusText: response.statusText,
              error: errorText,
            });
            toast.error('Upload Failed', `Could not upload recording: ${response.statusText}`);
          }
        } catch (error: any) {
          console.error('[RECORDING] ‚ùå Upload error:', error);
          toast.error('Upload Error', error.message || 'Failed to upload recording');
        } finally {
          // Clear recording state
          set({ callRecorder: null, isRecording: false, recordingChunks: [] });
        }
      };
      
      // Start recording with 1 second chunks (collects data every second)
      recorder.start(1000);
      set({ callRecorder: recorder, isRecording: true });
      console.log('[RECORDING] ‚úÖ Recording started successfully:', {
        state: recorder.state,
        mimeType: selectedMimeType,
        tracks: totalTracks,
      });
      toast.success('Recording Started', 'Call is being recorded');
      
    } catch (error: any) {
      console.error('[RECORDING] ‚ùå Failed to start recording:', {
        error: error.message,
        stack: error.stack,
      });
      toast.error('Recording Failed', error.message || 'Could not start recording');
      set({ callRecorder: null, isRecording: false });
    }
  },

  stopCallRecording: async () => {
    const { callRecorder } = get();
    
    if (!callRecorder) {
      console.log('[RECORDING] No active recording to stop');
      return;
    }
    
    if (callRecorder.state === 'inactive') {
      console.log('[RECORDING] Recording already stopped');
      set({ callRecorder: null, isRecording: false });
      return;
    }
    
    try {
      console.log('[RECORDING] ‚èπÔ∏è Stopping call recording, current state:', callRecorder.state);
      
      // Stop the recorder (this will trigger onstop event)
      callRecorder.stop();
      
      // Wait for onstop to complete (it handles the upload)
      // Give it enough time to process chunks and upload
      await new Promise(resolve => setTimeout(resolve, 2000));
      
      console.log('[RECORDING] ‚úÖ Recording stopped');
    } catch (error: any) {
      console.error('[RECORDING] ‚ùå Error stopping recording:', error);
      toast.error('Recording Error', 'Failed to stop recording properly');
      set({ callRecorder: null, isRecording: false, recordingChunks: [] });
    }
  },

  clearCall: async () => {
    // Stop speech recognition first
    get().stopSpeechRecognition();
    // Stop recording
    await get().stopCallRecording();
    
    // Make sure to stop any remaining tracks
    const { localStream, peerConnection, socket } = get();
    
    if (localStream) {
      localStream.getTracks().forEach(track => {
        track.stop();
      });
    }
    
    if (peerConnection) {
      peerConnection.close();
    }
    
    if (socket) {
      socket.disconnect();
    }
    
    set({
      socket: null,
      peerConnection: null,
      localStream: null,
      remoteStream: null,
      speechRecognition: null,
      callRecorder: null,
      recordingChunks: [],
      roomId: null,
      callId: null,
      userName: null,
      isHost: false,
      callStatus: 'idle',
      participants: [],
      callStartTime: null,
      transcript: [],
      interimTranscript: '',
      aiNotes: null,
      aiAnalysisInterval: null,
      isMuted: false,
      isVideoOff: false,
      isRecording: false,
      isMinimized: false,
      error: null,
    });
  },

  minimizeCall: () => {
    const { callStatus } = get();
    if (callStatus === 'active' || callStatus === 'waiting') {
      set({ isMinimized: true });
    }
  },

  maximizeCall: () => {
    set({ isMinimized: false });
  },
  };
});

